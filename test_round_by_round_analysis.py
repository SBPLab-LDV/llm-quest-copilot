#!/usr/bin/env python3
"""
DSPy é€è¼ªåˆ†ææ¸¬è©¦å·¥å…· (Round-by-Round Analysis Tool)

é€™å€‹å·¥å…·å°ˆé–€ç”¨æ–¼è©³ç´°åˆ†æ DSPy å°è©±æ¨¡çµ„åœ¨å¤šè¼ªå°è©±ä¸­çš„é€è¼ªè®ŠåŒ–ï¼Œ
å¹«åŠ©è­˜åˆ¥é€€åŒ–é–‹å§‹çš„ç²¾ç¢ºæ™‚é»å’ŒåŸå› ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. é€è¼ªå°è©±æ¸¬è©¦ï¼šè¨˜éŒ„æ¯è¼ªçš„è©³ç´°ç‹€æ…‹è®ŠåŒ–
2. æ¨ç†éç¨‹è¿½è¹¤ï¼šåˆ†ææ¨ç†å“è³ªåœ¨å„è¼ªçš„è®ŠåŒ–
3. é€€åŒ–æª¢æ¸¬ï¼šç²¾ç¢ºè­˜åˆ¥é€€åŒ–é–‹å§‹æ™‚é»
4. è¶¨å‹¢åˆ†æï¼šç”Ÿæˆè©³ç´°çš„å“è³ªè®ŠåŒ–å ±å‘Š
5. è¨ºæ–·å ±å‘Šï¼šè¼¸å‡ºçµæ§‹åŒ–çš„åˆ†æçµæœ

Author: DSPy Analysis Team  
Date: 2025-09-12
Version: 1.0.0
"""

import requests
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
import sys
import os

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('round_by_round_analysis.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# API é…ç½®
API_BASE_URL = "http://localhost:8000"
DIALOGUE_ENDPOINT = f"{API_BASE_URL}/api/dialogue/text"

@dataclass
class RoundAnalysis:
    """å–®è¼ªåˆ†æçµæœ"""
    round_number: int
    user_input: str
    api_response: Dict[str, Any]
    response_time: float
    analysis_results: Dict[str, Any]
    degradation_detected: bool
    quality_score: float
    timestamp: str


class RoundByRoundAnalyzer:
    """é€è¼ªåˆ†æå™¨ä¸»é¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.session_id = f"round_analysis_{int(time.time())}"
        self.round_analyses: List[RoundAnalysis] = []
        self.degradation_monitor = None
        
        # å°å…¥ç›£æ§ç³»çµ±
        try:
            sys.path.append('/app/src/core/dspy')
            from degradation_monitor import DegradationMonitor
            self.degradation_monitor = DegradationMonitor(enable_logging=True)
            logger.info("âœ… é€€åŒ–ç›£æ§ç³»çµ±å·²åŠ è¼‰")
        except ImportError as e:
            logger.warning(f"âš ï¸ ç„¡æ³•åŠ è¼‰é€€åŒ–ç›£æ§ç³»çµ±: {e}")
        
        # æ¨™æº–æ¸¬è©¦åºåˆ—
        self.test_sequence = [
            "ä½ å¥½ï¼Œæ„Ÿè¦ºæ€éº¼æ¨£ï¼Ÿ",
            "æœ‰æ²’æœ‰è¦ºå¾—ç™¼ç‡’æˆ–ä¸èˆ’æœï¼Ÿ", 
            "å¾ä»€éº¼æ™‚å€™é–‹å§‹çš„ï¼Ÿ",
            "é‚„æœ‰å…¶ä»–ç—‡ç‹€å—ï¼Ÿ",
            "é‚£æˆ‘å€‘å®‰æ’ä¸€äº›æª¢æŸ¥å¥½å—ï¼Ÿ"
        ]
        
        # æ¸¬è©¦è§’è‰²é…ç½®
        self.character_config = {
            "name": "Patient_1",
            "persona": "ä¸€ä½å‰›æ¥å—å£è…”ç™Œæ‰‹è¡“çš„ä¸­å¹´ç”·æ€§ç—…æ‚£ï¼Œè¡“å¾Œæ¢å¾©æœŸéœ€è¦å¯†åˆ‡é—œæ³¨èº«é«”ç‹€æ³ã€‚æ€§æ ¼æº«å’Œä½†ç•¥é¡¯ç„¦æ…®ã€‚",
            "backstory": "é™³å…ˆç”Ÿï¼Œ50æ­²ï¼Œå£è…”ç™Œç¬¬äºŒæœŸï¼Œå‰›å®Œæˆè…«ç˜¤åˆ‡é™¤æ‰‹è¡“ï¼Œç›®å‰åœ¨ä½é™¢è§€å¯ŸæœŸã€‚æ‰‹è¡“é †åˆ©ä½†ä»åœ¨æ¢å¾©ä¸­ã€‚",
            "goal": "é…åˆé†«å¸«æŸ¥æˆ¿ï¼Œå¦‚å¯¦åæ˜ èº«é«”ç‹€æ³ï¼Œå¸Œæœ›æ—©æ—¥åº·å¾©å‡ºé™¢ã€‚",
            "details": {
                "fixed_settings": {
                    "age": "50æ­²",
                    "condition": "å£è…”ç™Œè¡“å¾Œ",
                    "surgery_status": "å·²å®Œæˆè…«ç˜¤åˆ‡é™¤æ‰‹è¡“",
                    "current_status": "ä½é™¢è§€å¯ŸæœŸ"
                },
                "floating_settings": {
                    "mood": "ç•¥é¡¯ç„¦æ…®ä½†é…åˆæ²»ç™‚",
                    "pain_level": "è¼•åº¦åˆ°ä¸­åº¦ä¸é©",
                    "mobility": "å¯è‡ªç†ä½†éœ€è¦ä¼‘æ¯"
                }
            }
        }
        
        logger.info(f"ğŸ” é€è¼ªåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ - Session ID: {self.session_id}")
    
    def run_round_by_round_test(self) -> List[RoundAnalysis]:
        """åŸ·è¡Œé€è¼ªåˆ†ææ¸¬è©¦"""
        logger.info("ğŸš€ é–‹å§‹é€è¼ªå°è©±åˆ†ææ¸¬è©¦")
        logger.info(f"ğŸ“‹ æ¸¬è©¦åºåˆ—: {len(self.test_sequence)} è¼ªå°è©±")
        
        self.round_analyses.clear()
        
        for round_num, user_input in enumerate(self.test_sequence, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ”µ ç¬¬ {round_num} è¼ªåˆ†æé–‹å§‹")
            logger.info(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥: \"{user_input}\"")
            
            try:
                # åŸ·è¡Œå–®è¼ªåˆ†æ
                round_analysis = self._analyze_single_round(round_num, user_input)
                self.round_analyses.append(round_analysis)
                
                # è¨˜éŒ„åˆ†æçµæœ
                self._log_round_results(round_analysis)
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦ææ—©çµæŸ
                if round_analysis.degradation_detected and round_num >= 3:
                    logger.warning(f"ğŸš¨ ç¬¬ {round_num} è¼ªæª¢æ¸¬åˆ°åš´é‡é€€åŒ–ï¼Œç¹¼çºŒåˆ†æä»¥è§€å¯Ÿå¾ŒçºŒè®ŠåŒ–")
                
                # è¼ªæ¬¡é–“ç­‰å¾…
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ ç¬¬ {round_num} è¼ªåˆ†æå¤±æ•—: {str(e)}")
                continue
        
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ“Š é€è¼ªåˆ†ææ¸¬è©¦å®Œæˆ")
        
        # ç”Ÿæˆç¶œåˆåˆ†æå ±å‘Š
        self._generate_comprehensive_report()
        
        return self.round_analyses
    
    def _analyze_single_round(self, round_number: int, user_input: str) -> RoundAnalysis:
        """åˆ†æå–®è¼ªå°è©±"""
        
        # æº–å‚™ API è«‹æ±‚
        payload = {
            "text": user_input,
            "character_id": "Patient_1",
            "character_config": self.character_config
        }
        
        # åªåœ¨éé¦–è¼ªæ™‚åŠ å…¥ session_id
        if round_number > 1 and hasattr(self, 'actual_session_id'):
            payload["session_id"] = self.actual_session_id
        
        # è¨˜éŒ„è«‹æ±‚é–‹å§‹æ™‚é–“
        start_time = time.time()
        
        try:
            # ç™¼é€ API è«‹æ±‚
            logger.info(f"ğŸ“¤ ç™¼é€ API è«‹æ±‚...")
            response = requests.post(
                DIALOGUE_ENDPOINT,
                json=payload,
                headers={'Content-Type': 'application/json'},
                timeout=30
            )
            
            # è¨ˆç®—å›æ‡‰æ™‚é–“
            response_time = time.time() - start_time
            logger.info(f"â±ï¸ API å›æ‡‰æ™‚é–“: {response_time:.2f} ç§’")
            
            if response.status_code == 200:
                api_response = response.json()
                
                # ç¬¬ä¸€è¼ªæ™‚ä¿å­˜ session_id
                if round_number == 1 and 'session_id' in api_response:
                    self.actual_session_id = api_response['session_id']
                    logger.info(f"ğŸ“‹ ä¿å­˜æœƒè©± ID: {self.actual_session_id}")
                
                # åŸ·è¡Œè©³ç´°åˆ†æ
                analysis_results = self._perform_detailed_analysis(
                    api_response, round_number, user_input, response_time
                )
                
                # æª¢æ¸¬é€€åŒ–
                degradation_detected = self._detect_round_degradation(
                    api_response, analysis_results
                )
                
                # è¨ˆç®—å“è³ªåˆ†æ•¸
                quality_score = self._calculate_round_quality_score(
                    api_response, analysis_results
                )
                
                return RoundAnalysis(
                    round_number=round_number,
                    user_input=user_input,
                    api_response=api_response,
                    response_time=response_time,
                    analysis_results=analysis_results,
                    degradation_detected=degradation_detected,
                    quality_score=quality_score,
                    timestamp=datetime.now().isoformat()
                )
                
            else:
                logger.error(f"âŒ API è«‹æ±‚å¤±æ•—: {response.status_code} - {response.text}")
                raise Exception(f"API error: {response.status_code}")
                
        except requests.exceptions.Timeout:
            logger.error(f"â° API è«‹æ±‚è¶…æ™‚ (>{30}s)")
            raise Exception("API timeout")
        except Exception as e:
            logger.error(f"âŒ è«‹æ±‚éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
    
    def _perform_detailed_analysis(self, api_response: Dict[str, Any], 
                                 round_number: int, user_input: str, 
                                 response_time: float) -> Dict[str, Any]:
        """åŸ·è¡Œè©³ç´°åˆ†æ"""
        
        analysis = {
            'basic_info': {
                'response_count': len(api_response.get('responses', [])),
                'state': api_response.get('state', 'UNKNOWN'),
                'dialogue_context': api_response.get('dialogue_context', ''),
                'response_time': response_time,
                'has_reasoning': 'reasoning' in api_response
            },
            'content_analysis': {},
            'quality_metrics': {},
            'degradation_indicators': {},
            'reasoning_analysis': {}
        }
        
        # å…§å®¹åˆ†æ
        responses = api_response.get('responses', [])
        if responses:
            analysis['content_analysis'] = self._analyze_response_content(responses)
        
        # æ¨ç†åˆ†æ
        reasoning = api_response.get('reasoning', '')
        if reasoning:
            analysis['reasoning_analysis'] = self._analyze_reasoning_content(reasoning)
        
        # ä½¿ç”¨ç›£æ§ç³»çµ±é€²è¡Œå“è³ªè©•ä¼°
        if self.degradation_monitor:
            try:
                quality_metrics = self.degradation_monitor.assess_dialogue_quality(
                    api_response, round_number, None, self.character_config
                )
                analysis['quality_metrics'] = {
                    'overall_quality': quality_metrics.overall_quality_score,
                    'character_consistency': quality_metrics.character_consistency_score,
                    'response_relevance': quality_metrics.response_relevance_score,
                    'context_appropriateness': quality_metrics.context_appropriateness_score,
                    'reasoning_quality': quality_metrics.reasoning_quality_score,
                    'risk_level': quality_metrics.risk_level.value
                }
                
                analysis['degradation_indicators'] = {
                    'self_introduction_detected': quality_metrics.self_introduction_detected,
                    'generic_response_detected': quality_metrics.generic_response_detected,
                    'response_count_normal': quality_metrics.response_count_normal
                }
            except Exception as e:
                logger.warning(f"âš ï¸ ç›£æ§ç³»çµ±è©•ä¼°å¤±æ•—: {e}")
        
        return analysis
    
    def _analyze_response_content(self, responses: List[str]) -> Dict[str, Any]:
        """åˆ†æå›æ‡‰å…§å®¹"""
        
        analysis = {
            'total_responses': len(responses),
            'avg_length': sum(len(r) for r in responses) / len(responses) if responses else 0,
            'contains_medical_terms': False,
            'contains_self_introduction': False,
            'contains_generic_phrases': False,
            'response_diversity': 0
        }
        
        # æª¢æŸ¥é†«ç™‚ç›¸é—œè©å½™
        medical_terms = ['æ‰‹è¡“', 'å‚·å£', 'ç–¼ç—›', 'è…«è„¹', 'æ²»ç™‚', 'åº·å¾©', 'é†«å¸«', 'è­·ç†å¸«']
        for response in responses:
            if any(term in response for term in medical_terms):
                analysis['contains_medical_terms'] = True
                break
        
        # æª¢æŸ¥è‡ªæˆ‘ä»‹ç´¹æ¨¡å¼
        self_intro_patterns = ['æˆ‘æ˜¯Patient_1', 'æ‚¨å¥½ï¼Œæˆ‘æ˜¯', 'å£è…”ç™Œç—…æ‚£']
        for response in responses:
            if any(pattern in response for pattern in self_intro_patterns):
                analysis['contains_self_introduction'] = True
                break
        
        # æª¢æŸ¥é€šç”¨å›æ‡‰
        generic_patterns = ['æ²’æœ‰å®Œå…¨ç†è§£', 'æ›å€‹æ–¹å¼èªªæ˜', 'éœ€è¦ä»€éº¼å¹«åŠ©']
        for response in responses:
            if any(pattern in response for pattern in generic_patterns):
                analysis['contains_generic_phrases'] = True
                break
        
        # è¨ˆç®—å›æ‡‰å¤šæ¨£æ€§ï¼ˆç°¡å–®å­—ç¬¦ç›¸ä¼¼åº¦ï¼‰
        if len(responses) > 1:
            unique_chars = set(''.join(responses))
            total_chars = len(''.join(responses))
            analysis['response_diversity'] = len(unique_chars) / max(total_chars, 1)
        
        return analysis
    
    def _analyze_reasoning_content(self, reasoning: str) -> Dict[str, Any]:
        """åˆ†ææ¨ç†å…§å®¹"""
        
        analysis = {
            'length': len(reasoning),
            'contains_character_awareness': False,
            'contains_medical_context': False,
            'contains_systematic_thinking': False,
            'reasoning_depth_score': 0
        }
        
        # æª¢æŸ¥è§’è‰²æ„è­˜
        character_terms = ['ç—…æ‚£', 'è§’è‰²', 'Patient_1', 'å£è…”ç™Œ']
        if any(term in reasoning for term in character_terms):
            analysis['contains_character_awareness'] = True
        
        # æª¢æŸ¥é†«ç™‚æƒ…å¢ƒæ„è­˜
        medical_context_terms = ['é†«ç™‚', 'æ‰‹è¡“', 'è¡“å¾Œ', 'ç—‡ç‹€', 'æŸ¥æˆ¿']
        if any(term in reasoning for term in medical_context_terms):
            analysis['contains_medical_context'] = True
        
        # æª¢æŸ¥ç³»çµ±æ€§æ€è€ƒ
        systematic_patterns = ['è€ƒæ…®åˆ°', 'åŸºæ–¼', 'åˆ†æ', 'è©•ä¼°', 'æ ¹æ“š']
        if any(pattern in reasoning for pattern in systematic_patterns):
            analysis['contains_systematic_thinking'] = True
        
        # è¨ˆç®—æ¨ç†æ·±åº¦åˆ†æ•¸
        depth_indicators = [
            analysis['contains_character_awareness'],
            analysis['contains_medical_context'],
            analysis['contains_systematic_thinking'],
            len(reasoning) > 50,
            'å› ç‚º' in reasoning or 'æ‰€ä»¥' in reasoning
        ]
        analysis['reasoning_depth_score'] = sum(depth_indicators) / len(depth_indicators)
        
        return analysis
    
    def _detect_round_degradation(self, api_response: Dict[str, Any], 
                                analysis_results: Dict[str, Any]) -> bool:
        """æª¢æ¸¬è¼ªæ¬¡é€€åŒ–"""
        
        degradation_indicators = []
        
        # æª¢æŸ¥åŸºæœ¬é€€åŒ–æŒ‡æ¨™
        if analysis_results['basic_info']['response_count'] < 3:
            degradation_indicators.append('low_response_count')
        
        if analysis_results.get('content_analysis', {}).get('contains_self_introduction', False):
            degradation_indicators.append('self_introduction')
        
        if analysis_results.get('content_analysis', {}).get('contains_generic_phrases', False):
            degradation_indicators.append('generic_phrases')
        
        # æª¢æŸ¥å°è©±æƒ…å¢ƒé€€åŒ–
        dialogue_context = api_response.get('dialogue_context', '')
        if 'ä¸€èˆ¬å•è¨ºå°è©±' in dialogue_context:
            degradation_indicators.append('context_degradation')
        
        # æª¢æŸ¥æ¨ç†å“è³ª
        reasoning_analysis = analysis_results.get('reasoning_analysis', {})
        if reasoning_analysis.get('reasoning_depth_score', 0) < 0.3:
            degradation_indicators.append('poor_reasoning')
        
        # æª¢æŸ¥ç›£æ§ç³»çµ±çš„å“è³ªè©•ä¼°
        quality_metrics = analysis_results.get('quality_metrics', {})
        if quality_metrics.get('overall_quality', 1.0) < 0.5:
            degradation_indicators.append('low_overall_quality')
        
        is_degraded = len(degradation_indicators) >= 2
        
        if is_degraded:
            logger.warning(f"ğŸš¨ é€€åŒ–æª¢æ¸¬: {degradation_indicators}")
        
        return is_degraded
    
    def _calculate_round_quality_score(self, api_response: Dict[str, Any], 
                                     analysis_results: Dict[str, Any]) -> float:
        """è¨ˆç®—è¼ªæ¬¡å“è³ªåˆ†æ•¸"""
        
        # å¦‚æœæœ‰ç›£æ§ç³»çµ±çš„è©•ä¼°ï¼Œå„ªå…ˆä½¿ç”¨
        quality_metrics = analysis_results.get('quality_metrics', {})
        if quality_metrics.get('overall_quality') is not None:
            return quality_metrics['overall_quality']
        
        # å¦å‰‡ä½¿ç”¨ç°¡å–®è©•åˆ†
        score = 0.5  # åŸºç¤åˆ†æ•¸
        
        content_analysis = analysis_results.get('content_analysis', {})
        
        # å›æ‡‰æ•¸é‡è©•åˆ†
        response_count = analysis_results['basic_info']['response_count']
        if response_count >= 5:
            score += 0.2
        elif response_count >= 3:
            score += 0.1
        else:
            score -= 0.3
        
        # å…§å®¹å“è³ªè©•åˆ†
        if content_analysis.get('contains_medical_terms', False):
            score += 0.1
        
        if content_analysis.get('contains_self_introduction', False):
            score -= 0.4
        
        if content_analysis.get('contains_generic_phrases', False):
            score -= 0.2
        
        # æ¨ç†å“è³ªè©•åˆ†
        reasoning_analysis = analysis_results.get('reasoning_analysis', {})
        reasoning_score = reasoning_analysis.get('reasoning_depth_score', 0.5)
        score += reasoning_score * 0.2
        
        return min(max(score, 0.0), 1.0)
    
    def _log_round_results(self, round_analysis: RoundAnalysis):
        """è¨˜éŒ„è¼ªæ¬¡çµæœ"""
        
        logger.info(f"ğŸ“Š ç¬¬ {round_analysis.round_number} è¼ªåˆ†æçµæœ:")
        logger.info(f"  â±ï¸ å›æ‡‰æ™‚é–“: {round_analysis.response_time:.2f}s")
        logger.info(f"  ğŸ“ å›æ‡‰æ•¸é‡: {round_analysis.analysis_results['basic_info']['response_count']}")
        logger.info(f"  ğŸ¯ å“è³ªåˆ†æ•¸: {round_analysis.quality_score:.3f}")
        logger.info(f"  ğŸš¨ é€€åŒ–æª¢æ¸¬: {'æ˜¯' if round_analysis.degradation_detected else 'å¦'}")
        logger.info(f"  ğŸŒ å°è©±æƒ…å¢ƒ: {round_analysis.analysis_results['basic_info']['dialogue_context']}")
        
        # é¡¯ç¤ºå›æ‡‰å…§å®¹ï¼ˆå‰å…©å€‹ï¼‰
        responses = round_analysis.api_response.get('responses', [])
        logger.info(f"  ğŸ’¬ å›æ‡‰é è¦½:")
        for i, response in enumerate(responses[:2], 1):
            logger.info(f"    [{i}] {response}")
        if len(responses) > 2:
            logger.info(f"    ... å…± {len(responses)} å€‹å›æ‡‰")
        
        # å“è³ªæŒ‡æ¨™è©³æƒ…
        quality_metrics = round_analysis.analysis_results.get('quality_metrics', {})
        if quality_metrics:
            logger.info(f"  ğŸ“ˆ å“è³ªæŒ‡æ¨™:")
            for metric, value in quality_metrics.items():
                if isinstance(value, float):
                    logger.info(f"    {metric}: {value:.3f}")
                else:
                    logger.info(f"    {metric}: {value}")
    
    def _generate_comprehensive_report(self):
        """ç”Ÿæˆç¶œåˆåˆ†æå ±å‘Š"""
        
        logger.info(f"\n{'='*80}")
        logger.info("ğŸ“‹ DSPy é€è¼ªå°è©±ç¶œåˆåˆ†æå ±å‘Š")
        logger.info(f"{'='*80}")
        
        if not self.round_analyses:
            logger.warning("âš ï¸ ç„¡åˆ†ææ•¸æ“šï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Š")
            return
        
        # åŸºæœ¬çµ±è¨ˆ
        total_rounds = len(self.round_analyses)
        degraded_rounds = sum(1 for r in self.round_analyses if r.degradation_detected)
        avg_quality = sum(r.quality_score for r in self.round_analyses) / total_rounds
        
        logger.info(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        logger.info(f"  ğŸ”µ ç¸½è¼ªæ¬¡: {total_rounds}")
        logger.info(f"  ğŸ”´ é€€åŒ–è¼ªæ¬¡: {degraded_rounds}")
        logger.info(f"  ğŸ“ˆ å¹³å‡å“è³ª: {avg_quality:.3f}")
        logger.info(f"  ğŸ’¯ é€€åŒ–ç‡: {(degraded_rounds/total_rounds)*100:.1f}%")
        
        # è¼ªæ¬¡å“è³ªè¶¨å‹¢
        logger.info(f"\nğŸ“ˆ è¼ªæ¬¡å“è³ªè¶¨å‹¢:")
        for i, analysis in enumerate(self.round_analyses, 1):
            status = "ğŸ”´ é€€åŒ–" if analysis.degradation_detected else "ğŸŸ¢ æ­£å¸¸"
            logger.info(f"  ç¬¬{i}è¼ª: {analysis.quality_score:.3f} {status}")
        
        # é€€åŒ–æª¢æ¸¬ç¸½çµ
        if degraded_rounds > 0:
            first_degradation = next(
                (i+1 for i, r in enumerate(self.round_analyses) if r.degradation_detected),
                None
            )
            logger.info(f"\nğŸš¨ é€€åŒ–åˆ†æ:")
            logger.info(f"  ğŸ¯ é¦–æ¬¡é€€åŒ–: ç¬¬ {first_degradation} è¼ª")
            logger.info(f"  ğŸ“‰ é€€åŒ–æ¨¡å¼: æŒçºŒæ€§é€€åŒ–" if degraded_rounds > 1 else "  ğŸ“‰ é€€åŒ–æ¨¡å¼: å–®æ¬¡é€€åŒ–")
        
        # å›æ‡‰æ™‚é–“åˆ†æ
        response_times = [r.response_time for r in self.round_analyses]
        logger.info(f"\nâ±ï¸ å›æ‡‰æ™‚é–“åˆ†æ:")
        logger.info(f"  å¹³å‡: {sum(response_times)/len(response_times):.2f}s")
        logger.info(f"  æœ€å¿«: {min(response_times):.2f}s")
        logger.info(f"  æœ€æ…¢: {max(response_times):.2f}s")
        
        # å…§å®¹åˆ†ææ‘˜è¦
        self._generate_content_analysis_summary()
        
        # å»ºè­°å’Œçµè«–
        self._generate_recommendations()
        
        logger.info(f"{'='*80}")
    
    def _generate_content_analysis_summary(self):
        """ç”Ÿæˆå…§å®¹åˆ†ææ‘˜è¦"""
        
        logger.info(f"\nğŸ’¬ å…§å®¹åˆ†ææ‘˜è¦:")
        
        # çµ±è¨ˆå„é …æŒ‡æ¨™
        self_intro_count = sum(
            1 for r in self.round_analyses 
            if r.analysis_results.get('content_analysis', {}).get('contains_self_introduction', False)
        )
        
        generic_count = sum(
            1 for r in self.round_analyses
            if r.analysis_results.get('content_analysis', {}).get('contains_generic_phrases', False)
        )
        
        medical_terms_count = sum(
            1 for r in self.round_analyses
            if r.analysis_results.get('content_analysis', {}).get('contains_medical_terms', False)
        )
        
        logger.info(f"  ğŸ¥ é†«ç™‚è©å½™å‡ºç¾: {medical_terms_count}/{len(self.round_analyses)} è¼ª")
        logger.info(f"  ğŸ‘‹ è‡ªæˆ‘ä»‹ç´¹æ¨¡å¼: {self_intro_count}/{len(self.round_analyses)} è¼ª")
        logger.info(f"  ğŸ¤– é€šç”¨å›æ‡‰æ¨¡å¼: {generic_count}/{len(self.round_analyses)} è¼ª")
        
        # å›æ‡‰æ•¸é‡çµ±è¨ˆ
        response_counts = [
            r.analysis_results['basic_info']['response_count'] 
            for r in self.round_analyses
        ]
        avg_responses = sum(response_counts) / len(response_counts)
        logger.info(f"  ğŸ“ å¹³å‡å›æ‡‰æ•¸é‡: {avg_responses:.1f}")
    
    def _generate_recommendations(self):
        """ç”Ÿæˆå»ºè­°å’Œçµè«–"""
        
        logger.info(f"\nğŸ’¡ åˆ†æå»ºè­°:")
        
        degraded_rounds = sum(1 for r in self.round_analyses if r.degradation_detected)
        
        if degraded_rounds == 0:
            logger.info("  âœ… æœªæª¢æ¸¬åˆ°é€€åŒ–å•é¡Œï¼Œå°è©±å“è³ªä¿æŒç©©å®š")
        else:
            first_degradation = next(
                (i+1 for i, r in enumerate(self.round_analyses) if r.degradation_detected),
                None
            )
            
            logger.info(f"  ğŸš¨ æª¢æ¸¬åˆ°é€€åŒ–å•é¡Œï¼Œé¦–æ¬¡å‡ºç¾åœ¨ç¬¬ {first_degradation} è¼ª")
            
            if first_degradation and first_degradation >= 4:
                logger.info("  ğŸ“‹ å»ºè­°é‡é»é—œæ³¨ç¬¬4-5è¼ªçš„é€€åŒ–é é˜²æ©Ÿåˆ¶")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è‡ªæˆ‘ä»‹ç´¹æ¨¡å¼
            self_intro_detected = any(
                r.analysis_results.get('content_analysis', {}).get('contains_self_introduction', False)
                for r in self.round_analyses
            )
            
            if self_intro_detected:
                logger.info("  ğŸ”§ å»ºè­°åŠ å¼·è§’è‰²ä¸€è‡´æ€§æª¢æŸ¥æ©Ÿåˆ¶")
            
            # æª¢æŸ¥å›æ‡‰æ•¸é‡å•é¡Œ
            low_response_rounds = sum(
                1 for r in self.round_analyses 
                if r.analysis_results['basic_info']['response_count'] < 3
            )
            
            if low_response_rounds > 0:
                logger.info("  ğŸ“ å»ºè­°å„ªåŒ–å›æ‡‰ç”Ÿæˆæ•¸é‡æ§åˆ¶")
        
        # ä¿å­˜è©³ç´°å ±å‘Šåˆ°æ–‡ä»¶
        self._save_detailed_report()
    
    def _save_detailed_report(self):
        """ä¿å­˜è©³ç´°å ±å‘Šåˆ°æ–‡ä»¶"""
        
        report_filename = f"round_by_round_analysis_report_{int(time.time())}.json"
        
        report_data = {
            'session_id': self.session_id,
            'timestamp': datetime.now().isoformat(),
            'test_sequence': self.test_sequence,
            'character_config': self.character_config,
            'summary': {
                'total_rounds': len(self.round_analyses),
                'degraded_rounds': sum(1 for r in self.round_analyses if r.degradation_detected),
                'average_quality': sum(r.quality_score for r in self.round_analyses) / len(self.round_analyses) if self.round_analyses else 0,
                'first_degradation_round': next(
                    (i+1 for i, r in enumerate(self.round_analyses) if r.degradation_detected),
                    None
                )
            },
            'round_analyses': [
                {
                    'round_number': r.round_number,
                    'user_input': r.user_input,
                    'response_time': r.response_time,
                    'quality_score': r.quality_score,
                    'degradation_detected': r.degradation_detected,
                    'analysis_results': r.analysis_results,
                    'api_response': r.api_response,
                    'timestamp': r.timestamp
                }
                for r in self.round_analyses
            ]
        }
        
        try:
            with open(report_filename, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜: {report_filename}")
            
        except Exception as e:
            logger.error(f"âŒ å ±å‘Šä¿å­˜å¤±æ•—: {str(e)}")


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ” DSPy é€è¼ªåˆ†ææ¸¬è©¦å·¥å…·")
    print("=" * 50)
    
    analyzer = RoundByRoundAnalyzer()
    
    try:
        # åŸ·è¡Œé€è¼ªåˆ†ææ¸¬è©¦
        analyses = analyzer.run_round_by_round_test()
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼å…±åˆ†æ {len(analyses)} è¼ªå°è©±")
        print("ğŸ“„ è©³ç´°æ—¥èªŒè«‹æŸ¥çœ‹: round_by_round_analysis.log")
        
        return analyses
        
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        logger.error(f"æ¸¬è©¦å¤±æ•—: {str(e)}", exc_info=True)
    
    return []


if __name__ == "__main__":
    main()