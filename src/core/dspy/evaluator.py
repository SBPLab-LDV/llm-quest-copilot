"""
DSPy è©•ä¼°å™¨

å¯¦ç¾å°è©±å“è³ªè©•ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬å›æ‡‰å“è³ªã€æƒ…å¢ƒæº–ç¢ºåº¦å’Œå°è©±é€£è²«æ€§è©•ä¼°ã€‚
æ”¯æ´å¤šç¨®è©•ä¼°æŒ‡æ¨™å’Œè‡ªå‹•åŒ–è©•ä¼°æµç¨‹ã€‚
"""

import dspy
from typing import List, Dict, Any, Optional, Tuple, Union, Callable
import logging
import json
import numpy as np
from datetime import datetime
from collections import defaultdict, Counter
import re

# é¿å…ç›¸å°å°å…¥å•é¡Œ
try:
    from .signatures import ResponseEvaluationSignature
    from .dialogue_module import DSPyDialogueModule
    from .config import DSPyConfig
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(__file__))
    from signatures import ResponseEvaluationSignature
    from dialogue_module import DSPyDialogueModule
    from config import DSPyConfig

logger = logging.getLogger(__name__)

class DSPyEvaluator:
    """DSPy è©•ä¼°å™¨
    
    æä¾›å¤šç¶­åº¦çš„å°è©±ç³»çµ±è©•ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬è‡ªå‹•åŒ–å’ŒåŸºæ–¼ DSPy çš„è©•ä¼°ã€‚
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–è©•ä¼°å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config_manager = DSPyConfig()
        self.config = config or self.config_manager.get_dspy_config()
        
        # DSPy è©•ä¼°æ¨¡çµ„ (éœ€è¦ LM æ”¯æ´æ™‚ä½¿ç”¨)
        self.response_evaluator = None
        
        # è©•ä¼°æŒ‡æ¨™
        self.metrics = {
            'response_quality': self._evaluate_response_quality,
            'context_accuracy': self._evaluate_context_accuracy,
            'dialogue_coherence': self._evaluate_dialogue_coherence,
            'state_consistency': self._evaluate_state_consistency,
            'diversity_score': self._evaluate_diversity,
            'safety_score': self._evaluate_safety
        }
        
        # è©•ä¼°æ­·å²
        self.evaluation_history: List[Dict[str, Any]] = []
        
        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            'total_evaluations': 0,
            'average_scores': {},
            'evaluation_types': Counter(),
            'last_evaluation': None
        }
        
        logger.info("DSPy è©•ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def evaluate_prediction(self, 
                           user_input: str,
                           prediction: dspy.Prediction,
                           expected_output: Optional[Dict[str, Any]] = None,
                           evaluation_metrics: List[str] = None) -> Dict[str, Any]:
        """è©•ä¼°å–®å€‹é æ¸¬çµæœ
        
        Args:
            user_input: ç”¨æˆ¶è¼¸å…¥
            prediction: æ¨¡å‹é æ¸¬çµæœ
            expected_output: é æœŸè¼¸å‡º (å¯é¸)
            evaluation_metrics: è¦ä½¿ç”¨çš„è©•ä¼°æŒ‡æ¨™åˆ—è¡¨
            
        Returns:
            è©•ä¼°çµæœå­—å…¸
        """
        try:
            # ä½¿ç”¨æ‰€æœ‰æŒ‡æ¨™å¦‚æœæ²’æœ‰æŒ‡å®š
            if evaluation_metrics is None:
                evaluation_metrics = list(self.metrics.keys())
            
            evaluation_result = {
                'user_input': user_input,
                'prediction': self._prediction_to_dict(prediction),
                'expected_output': expected_output,
                'timestamp': datetime.now().isoformat(),
                'scores': {},
                'overall_score': 0.0
            }
            
            # åŸ·è¡Œå„é …è©•ä¼°
            total_score = 0.0
            successful_metrics = 0
            
            for metric_name in evaluation_metrics:
                if metric_name in self.metrics:
                    try:
                        score = self.metrics[metric_name](
                            user_input, prediction, expected_output
                        )
                        evaluation_result['scores'][metric_name] = score
                        total_score += score
                        successful_metrics += 1
                        
                    except Exception as e:
                        logger.error(f"è©•ä¼°æŒ‡æ¨™ {metric_name} å¤±æ•—: {e}")
                        evaluation_result['scores'][metric_name] = 0.0
                else:
                    logger.warning(f"æœªçŸ¥çš„è©•ä¼°æŒ‡æ¨™: {metric_name}")
            
            # è¨ˆç®—ç¸½åˆ†
            if successful_metrics > 0:
                evaluation_result['overall_score'] = total_score / successful_metrics
            
            # æ›´æ–°çµ±è¨ˆ
            self._update_evaluation_stats(evaluation_result)
            
            # è¨˜éŒ„è©•ä¼°æ­·å²
            self.evaluation_history.append(evaluation_result)
            
            logger.debug(f"è©•ä¼°å®Œæˆï¼Œç¸½åˆ†: {evaluation_result['overall_score']:.2f}")
            return evaluation_result
            
        except Exception as e:
            logger.error(f"è©•ä¼°å¤±æ•—: {e}")
            return {
                'user_input': user_input,
                'error': str(e),
                'timestamp': datetime.now().isoformat(),
                'scores': {},
                'overall_score': 0.0
            }
    
    def batch_evaluate(self, 
                      test_cases: List[Dict[str, Any]],
                      model: DSPyDialogueModule,
                      evaluation_metrics: List[str] = None) -> Dict[str, Any]:
        """æ‰¹é‡è©•ä¼°æ¸¬è©¦æ¡ˆä¾‹
        
        Args:
            test_cases: æ¸¬è©¦æ¡ˆä¾‹åˆ—è¡¨ï¼Œæ¯å€‹åŒ…å«è¼¸å…¥åƒæ•¸
            model: è¦è©•ä¼°çš„å°è©±æ¨¡çµ„
            evaluation_metrics: è©•ä¼°æŒ‡æ¨™åˆ—è¡¨
            
        Returns:
            æ‰¹é‡è©•ä¼°çµæœ
        """
        try:
            logger.info(f"é–‹å§‹æ‰¹é‡è©•ä¼° {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹...")
            
            batch_results = {
                'total_cases': len(test_cases),
                'successful_cases': 0,
                'failed_cases': 0,
                'individual_results': [],
                'aggregate_scores': {},
                'timestamp': datetime.now().isoformat()
            }
            
            all_scores = defaultdict(list)
            
            for i, test_case in enumerate(test_cases):
                try:
                    # åŸ·è¡Œé æ¸¬
                    prediction = model(**test_case)
                    
                    # è©•ä¼°é æ¸¬çµæœ
                    evaluation_result = self.evaluate_prediction(
                        user_input=test_case.get('user_input', ''),
                        prediction=prediction,
                        expected_output=test_case.get('expected_output'),
                        evaluation_metrics=evaluation_metrics
                    )
                    
                    batch_results['individual_results'].append(evaluation_result)
                    batch_results['successful_cases'] += 1
                    
                    # ç´¯ç©åˆ†æ•¸
                    for metric, score in evaluation_result['scores'].items():
                        all_scores[metric].append(score)
                    all_scores['overall'].append(evaluation_result['overall_score'])
                    
                except Exception as e:
                    logger.error(f"æ¸¬è©¦æ¡ˆä¾‹ {i} è©•ä¼°å¤±æ•—: {e}")
                    batch_results['failed_cases'] += 1
                    batch_results['individual_results'].append({
                        'test_case_index': i,
                        'error': str(e),
                        'overall_score': 0.0
                    })
            
            # è¨ˆç®—èšåˆåˆ†æ•¸
            for metric, scores in all_scores.items():
                if scores:
                    batch_results['aggregate_scores'][metric] = {
                        'mean': np.mean(scores),
                        'std': np.std(scores),
                        'min': np.min(scores),
                        'max': np.max(scores),
                        'count': len(scores)
                    }
            
            logger.info(f"æ‰¹é‡è©•ä¼°å®Œæˆ: {batch_results['successful_cases']}/{batch_results['total_cases']} æˆåŠŸ")
            return batch_results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡è©•ä¼°å¤±æ•—: {e}")
            return {
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _evaluate_response_quality(self, 
                                  user_input: str, 
                                  prediction: dspy.Prediction,
                                  expected_output: Optional[Dict[str, Any]] = None) -> float:
        """è©•ä¼°å›æ‡‰å“è³ª
        
        è©•ä¼°ç”Ÿæˆçš„å›æ‡‰æ˜¯å¦åˆç†ã€å®Œæ•´å’Œæœ‰ç”¨ã€‚
        """
        try:
            score = 0.0
            
            # æª¢æŸ¥å›æ‡‰å­˜åœ¨æ€§å’Œæ•¸é‡
            if hasattr(prediction, 'responses') and prediction.responses:
                responses = prediction.responses
                
                # å›æ‡‰æ•¸é‡æª¢æŸ¥ (æœŸæœ›3-5å€‹)
                if len(responses) >= 3:
                    score += 0.2
                
                # å›æ‡‰é•·åº¦å’Œå…§å®¹æª¢æŸ¥
                for response in responses:
                    if isinstance(response, str):
                        response_len = len(response.strip())
                        
                        # é•·åº¦åˆç† (5-100å­—)
                        if 5 <= response_len <= 100:
                            score += 0.1
                        
                        # é¿å…é‡è¤‡
                        if response.count(response[:10]) <= 1:  # ç°¡å–®é‡è¤‡æª¢æŸ¥
                            score += 0.05
                        
                        # åŒ…å«é©ç•¶çš„æ¨™é»ç¬¦è™Ÿ
                        if any(punct in response for punct in ['ã€‚', 'ï¼Ÿ', 'ï¼', 'ï¼Œ']):
                            score += 0.05
            
            # æƒ…å¢ƒç›¸é—œæ€§æª¢æŸ¥
            if hasattr(prediction, 'dialogue_context') and prediction.dialogue_context:
                context = prediction.dialogue_context.strip()
                if len(context) > 0:
                    score += 0.1
            
            return min(score, 1.0)
            
        except Exception as e:
            logger.error(f"å›æ‡‰å“è³ªè©•ä¼°å¤±æ•—: {e}")
            return 0.0
    
    def _evaluate_context_accuracy(self,
                                  user_input: str,
                                  prediction: dspy.Prediction,
                                  expected_output: Optional[Dict[str, Any]] = None) -> float:
        """è©•ä¼°æƒ…å¢ƒæº–ç¢ºåº¦
        
        è©•ä¼°è­˜åˆ¥çš„å°è©±æƒ…å¢ƒæ˜¯å¦æº–ç¢ºã€‚
        """
        try:
            score = 0.0
            
            # å¦‚æœæœ‰é æœŸæƒ…å¢ƒï¼Œç›´æ¥æ¯”è¼ƒ
            if expected_output and 'dialogue_context' in expected_output:
                expected_context = expected_output['dialogue_context']
                predicted_context = getattr(prediction, 'dialogue_context', '')
                
                if predicted_context == expected_context:
                    score = 1.0
                elif self._contexts_similar(predicted_context, expected_context):
                    score = 0.7
                else:
                    score = 0.0
            else:
                # åŸºæ–¼å•Ÿç™¼å¼è¦å‰‡è©•ä¼°æƒ…å¢ƒåˆç†æ€§
                predicted_context = getattr(prediction, 'dialogue_context', '')
                
                if predicted_context:
                    # æª¢æŸ¥æƒ…å¢ƒèˆ‡ç”¨æˆ¶è¼¸å…¥çš„ç›¸é—œæ€§
                    relevance_score = self._calculate_context_relevance(
                        user_input, predicted_context
                    )
                    score = relevance_score
            
            return score
            
        except Exception as e:
            logger.error(f"æƒ…å¢ƒæº–ç¢ºåº¦è©•ä¼°å¤±æ•—: {e}")
            return 0.0
    
    def _evaluate_dialogue_coherence(self,
                                   user_input: str,
                                   prediction: dspy.Prediction,
                                   expected_output: Optional[Dict[str, Any]] = None) -> float:
        """è©•ä¼°å°è©±é€£è²«æ€§
        
        è©•ä¼°å›æ‡‰æ˜¯å¦èˆ‡ç”¨æˆ¶è¼¸å…¥é‚è¼¯é€£è²«ã€‚
        """
        try:
            score = 0.0
            
            if not hasattr(prediction, 'responses') or not prediction.responses:
                return 0.0
            
            # æª¢æŸ¥å›æ‡‰èˆ‡ç”¨æˆ¶è¼¸å…¥çš„é‚è¼¯é—œè¯
            user_input_lower = user_input.lower()
            
            for response in prediction.responses:
                if isinstance(response, str):
                    # æª¢æŸ¥é—œéµè©é‡ç–Š
                    user_words = set(user_input_lower.split())
                    response_words = set(response.lower().split())
                    
                    # è¨ˆç®—è©å½™ç›¸é—œæ€§
                    if user_words and response_words:
                        overlap = len(user_words & response_words)
                        relevance = overlap / max(len(user_words), len(response_words))
                        score += relevance * 0.2
                    
                    # æª¢æŸ¥å›æ‡‰çš„é©ç•¶æ€§
                    if self._is_appropriate_response(user_input, response):
                        score += 0.1
            
            return min(score, 1.0)
            
        except Exception as e:
            logger.error(f"å°è©±é€£è²«æ€§è©•ä¼°å¤±æ•—: {e}")
            return 0.0
    
    def _evaluate_state_consistency(self,
                                  user_input: str,
                                  prediction: dspy.Prediction,
                                  expected_output: Optional[Dict[str, Any]] = None) -> float:
        """è©•ä¼°ç‹€æ…‹ä¸€è‡´æ€§
        
        è©•ä¼°å°è©±ç‹€æ…‹æ˜¯å¦åˆç†ã€‚
        """
        try:
            predicted_state = getattr(prediction, 'state', '')
            valid_states = ['NORMAL', 'CONFUSED', 'TRANSITIONING', 'TERMINATED']
            
            # æª¢æŸ¥ç‹€æ…‹æœ‰æ•ˆæ€§
            if predicted_state not in valid_states:
                return 0.0
            
            score = 0.5  # åŸºç¤åˆ†æ•¸ï¼Œç‹€æ…‹æœ‰æ•ˆ
            
            # å¦‚æœæœ‰é æœŸç‹€æ…‹ï¼Œæ¯”è¼ƒä¸€è‡´æ€§
            if expected_output and 'state' in expected_output:
                expected_state = expected_output['state']
                if predicted_state == expected_state:
                    score = 1.0
                else:
                    score = 0.3  # ç‹€æ…‹ä¸åŒ¹é…ä½†ä»æœ‰æ•ˆ
            else:
                # åŸºæ–¼å•Ÿç™¼å¼è¦å‰‡è©•ä¼°ç‹€æ…‹åˆç†æ€§
                state_score = self._evaluate_state_appropriateness(
                    user_input, predicted_state
                )
                score = max(score, state_score)
            
            return score
            
        except Exception as e:
            logger.error(f"ç‹€æ…‹ä¸€è‡´æ€§è©•ä¼°å¤±æ•—: {e}")
            return 0.0
    
    def _evaluate_diversity(self,
                          user_input: str,
                          prediction: dspy.Prediction,
                          expected_output: Optional[Dict[str, Any]] = None) -> float:
        """è©•ä¼°å›æ‡‰å¤šæ¨£æ€§
        
        è©•ä¼°ç”Ÿæˆçš„å›æ‡‰é¸é …æ˜¯å¦å…·æœ‰å¤šæ¨£æ€§ã€‚
        """
        try:
            if not hasattr(prediction, 'responses') or not prediction.responses:
                return 0.0
            
            responses = [str(r) for r in prediction.responses if r]
            
            if len(responses) < 2:
                return 0.0
            
            # è¨ˆç®—å›æ‡‰é–“çš„å·®ç•°æ€§
            diversity_scores = []
            
            for i in range(len(responses)):
                for j in range(i + 1, len(responses)):
                    similarity = self._calculate_text_similarity(responses[i], responses[j])
                    diversity_scores.append(1 - similarity)  # å¤šæ¨£æ€§ = 1 - ç›¸ä¼¼æ€§
            
            if diversity_scores:
                average_diversity = np.mean(diversity_scores)
                return min(average_diversity, 1.0)
            
            return 0.0
            
        except Exception as e:
            logger.error(f"å¤šæ¨£æ€§è©•ä¼°å¤±æ•—: {e}")
            return 0.0
    
    def _evaluate_safety(self,
                        user_input: str,
                        prediction: dspy.Prediction,
                        expected_output: Optional[Dict[str, Any]] = None) -> float:
        """è©•ä¼°å›æ‡‰å®‰å…¨æ€§
        
        æª¢æŸ¥å›æ‡‰æ˜¯å¦åŒ…å«ä¸ç•¶å…§å®¹ã€‚
        """
        try:
            score = 1.0  # é è¨­å®‰å…¨
            
            if not hasattr(prediction, 'responses') or not prediction.responses:
                return score
            
            # ä¸ç•¶å…§å®¹é—œéµè© (ç°¡åŒ–ç‰ˆæœ¬)
            unsafe_keywords = [
                'æ­»', 'è‡ªæ®º', 'å‚·å®³', 'æš´åŠ›', 'ä»‡æ¨',
                'æ­§è¦–', 'ä¸ç•¶', 'é•æ³•', 'å±éšª'
            ]
            
            for response in prediction.responses:
                if isinstance(response, str):
                    response_lower = response.lower()
                    for keyword in unsafe_keywords:
                        if keyword in response_lower:
                            score -= 0.2
                            break
            
            return max(score, 0.0)
            
        except Exception as e:
            logger.error(f"å®‰å…¨æ€§è©•ä¼°å¤±æ•—: {e}")
            return 1.0  # è©•ä¼°å¤±æ•—æ™‚é è¨­å®‰å…¨
    
    def _prediction_to_dict(self, prediction: dspy.Prediction) -> Dict[str, Any]:
        """å°‡é æ¸¬çµæœè½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        try:
            result = {}
            
            # æå–å¸¸è¦‹å±¬æ€§
            common_attrs = [
                'responses', 'state', 'dialogue_context', 
                'confidence', 'context_classification'
            ]
            
            for attr in common_attrs:
                if hasattr(prediction, attr):
                    value = getattr(prediction, attr)
                    # ç¢ºä¿å¯åºåˆ—åŒ–
                    if isinstance(value, (str, int, float, list, dict, bool)):
                        result[attr] = value
                    else:
                        result[attr] = str(value)
            
            return result
            
        except Exception as e:
            logger.error(f"é æ¸¬çµæœè½‰æ›å¤±æ•—: {e}")
            return {'error': str(e)}
    
    def _contexts_similar(self, context1: str, context2: str) -> bool:
        """æª¢æŸ¥å…©å€‹æƒ…å¢ƒæ˜¯å¦ç›¸ä¼¼"""
        if not context1 or not context2:
            return False
        
        # ç°¡å–®çš„ç›¸ä¼¼æ€§æª¢æŸ¥
        words1 = set(context1.lower().split())
        words2 = set(context2.lower().split())
        
        if words1 and words2:
            overlap = len(words1 & words2)
            total = len(words1 | words2)
            similarity = overlap / total
            return similarity > 0.5
        
        return False
    
    def _calculate_context_relevance(self, user_input: str, context: str) -> float:
        """è¨ˆç®—æƒ…å¢ƒèˆ‡ç”¨æˆ¶è¼¸å…¥çš„ç›¸é—œæ€§"""
        try:
            # é—œéµè©æ˜ å°„
            context_keywords = {
                'ç”Ÿå‘½å¾µè±¡': ['è¡€å£“', 'é«”æº«', 'å¿ƒè·³', 'å‘¼å¸', 'è¡€æ°§'],
                'å‚·å£ç®¡è·¯': ['å‚·å£', 'ç®¡è·¯', 'å°ç®¡', 'æ›è—¥', 'æ‹†ç·š'],
                'å¾©å¥': ['å¾©å¥', 'é‹å‹•', 'æ´»å‹•', 'ç‰©ç†æ²»ç™‚'],
                'ç‡Ÿé¤Š': ['åƒ', 'é£²é£Ÿ', 'ç‡Ÿé¤Š', 'é¤µé£Ÿ'],
                'æ—¥å¸¸': ['ç¡è¦º', 'ä¼‘æ¯', 'æ´—æ¾¡', 'ä¸Šå»æ‰€']
            }
            
            user_lower = user_input.lower()
            context_lower = context.lower()
            
            # ç›´æ¥åŒ¹é…
            for category, keywords in context_keywords.items():
                if category in context_lower:
                    for keyword in keywords:
                        if keyword in user_lower:
                            return 0.8
            
            # è©å½™é‡ç–Š
            user_words = set(user_lower.split())
            context_words = set(context_lower.split())
            
            if user_words and context_words:
                overlap = len(user_words & context_words)
                total = len(user_words | context_words)
                return overlap / total
            
            return 0.3  # é è¨­ç›¸é—œæ€§
            
        except Exception:
            return 0.3
    
    def _is_appropriate_response(self, user_input: str, response: str) -> bool:
        """æª¢æŸ¥å›æ‡‰æ˜¯å¦é©ç•¶"""
        try:
            # ç°¡å–®çš„é©ç•¶æ€§æª¢æŸ¥
            user_lower = user_input.lower()
            response_lower = response.lower()
            
            # å¦‚æœæ˜¯å•å€™ï¼Œå›æ‡‰æ‡‰è©²ä¹Ÿæ˜¯å•å€™
            if any(greeting in user_lower for greeting in ['ä½ å¥½', 'æ—©å®‰', 'æ™šå®‰']):
                if any(greeting in response_lower for greeting in ['ä½ å¥½', 'æ—©å®‰', 'æ™šå®‰', 'å“ˆå›‰']):
                    return True
            
            # å¦‚æœæ˜¯ç–¼ç—›ç›¸é—œï¼Œå›æ‡‰æ‡‰è©²ç›¸é—œ
            if any(pain in user_lower for pain in ['ç—›', 'ç–¼', 'ä¸èˆ’æœ']):
                if any(pain_resp in response_lower for pain_resp in ['ç—›', 'ç–¼', 'ä¸èˆ’æœ', 'é‚„å¥½']):
                    return True
            
            # é è¨­é©ç•¶
            return True
            
        except Exception:
            return True
    
    def _evaluate_state_appropriateness(self, user_input: str, predicted_state: str) -> float:
        """è©•ä¼°ç‹€æ…‹çš„é©ç•¶æ€§"""
        try:
            user_lower = user_input.lower()
            
            # å›°æƒ‘ç‹€æ…‹çš„è§¸ç™¼è©
            confusion_triggers = ['ä¸æ‡‚', 'ä»€éº¼', 'ä¸æ˜ç™½', 'æä¸æ¸…æ¥š']
            
            # çµæŸç‹€æ…‹çš„è§¸ç™¼è©  
            termination_triggers = ['çµæŸ', 'å†è¦‹', 'è¬è¬', 'æ°æ°']
            
            if predicted_state == 'CONFUSED':
                if any(trigger in user_lower for trigger in confusion_triggers):
                    return 0.9
                else:
                    return 0.4
            elif predicted_state == 'TERMINATED':
                if any(trigger in user_lower for trigger in termination_triggers):
                    return 0.9
                else:
                    return 0.3
            elif predicted_state == 'NORMAL':
                return 0.7  # å¤§éƒ¨åˆ†æƒ…æ³ä¸‹æ­£å¸¸ç‹€æ…‹éƒ½æ˜¯åˆç†çš„
            else:
                return 0.5
                
        except Exception:
            return 0.5
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è¨ˆç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        try:
            if not text1 or not text2:
                return 0.0
            
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            
            if not words1 or not words2:
                return 0.0
            
            intersection = len(words1 & words2)
            union = len(words1 | words2)
            
            return intersection / union if union > 0 else 0.0
            
        except Exception:
            return 0.0
    
    def _update_evaluation_stats(self, evaluation_result: Dict[str, Any]):
        """æ›´æ–°è©•ä¼°çµ±è¨ˆè³‡è¨Š"""
        try:
            self.stats['total_evaluations'] += 1
            self.stats['last_evaluation'] = evaluation_result['timestamp']
            
            # æ›´æ–°å¹³å‡åˆ†æ•¸
            for metric, score in evaluation_result['scores'].items():
                if metric not in self.stats['average_scores']:
                    self.stats['average_scores'][metric] = []
                self.stats['average_scores'][metric].append(score)
            
            # æ›´æ–°è©•ä¼°é¡å‹çµ±è¨ˆ
            metrics_used = list(evaluation_result['scores'].keys())
            self.stats['evaluation_types'][','.join(sorted(metrics_used))] += 1
            
        except Exception as e:
            logger.error(f"æ›´æ–°çµ±è¨ˆå¤±æ•—: {e}")
    
    def get_evaluation_statistics(self) -> Dict[str, Any]:
        """ç²å–è©•ä¼°çµ±è¨ˆè³‡è¨Š"""
        try:
            stats = self.stats.copy()
            
            # è¨ˆç®—å¹³å‡åˆ†æ•¸
            if self.stats['average_scores']:
                stats['current_averages'] = {}
                for metric, scores in self.stats['average_scores'].items():
                    if scores:
                        stats['current_averages'][metric] = {
                            'mean': np.mean(scores),
                            'std': np.std(scores) if len(scores) > 1 else 0.0,
                            'count': len(scores)
                        }
            
            return stats
            
        except Exception as e:
            logger.error(f"ç²å–çµ±è¨ˆå¤±æ•—: {e}")
            return {}
    
    def get_recent_evaluations(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ç²å–æœ€è¿‘çš„è©•ä¼°è¨˜éŒ„"""
        return self.evaluation_history[-limit:] if self.evaluation_history else []

# ä¾¿åˆ©å‡½æ•¸
def create_evaluator(config: Optional[Dict[str, Any]] = None) -> DSPyEvaluator:
    """å‰µå»ºè©•ä¼°å™¨å¯¦ä¾‹"""
    return DSPyEvaluator(config)

# æ¸¬è©¦å‡½æ•¸  
def test_evaluator():
    """æ¸¬è©¦è©•ä¼°å™¨åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦ DSPy è©•ä¼°å™¨...")
    
    try:
        # å‰µå»ºè©•ä¼°å™¨
        print("\n1. å‰µå»ºè©•ä¼°å™¨:")
        evaluator = DSPyEvaluator()
        print("  âœ… è©•ä¼°å™¨å‰µå»ºæˆåŠŸ")
        
        # å‰µå»ºæ¨¡æ“¬é æ¸¬çµæœ
        print("\n2. æ¸¬è©¦å–®å€‹é æ¸¬è©•ä¼°:")
        mock_prediction = type('MockPrediction', (), {
            'responses': ['æˆ‘å¾ˆå¥½', 'ä»Šå¤©æ„Ÿè¦ºä¸éŒ¯', 'è¬è¬é—œå¿ƒ'],
            'state': 'NORMAL', 
            'dialogue_context': 'ä¸€èˆ¬å°è©±'
        })()
        
        evaluation_result = evaluator.evaluate_prediction(
            user_input="ä½ ä»Šå¤©æ„Ÿè¦ºå¦‚ä½•ï¼Ÿ",
            prediction=mock_prediction
        )
        
        print(f"  âœ… è©•ä¼°å®Œæˆï¼Œç¸½åˆ†: {evaluation_result['overall_score']:.2f}")
        print(f"  è©•ä¼°æŒ‡æ¨™: {list(evaluation_result['scores'].keys())}")
        
        # æ¸¬è©¦çµ±è¨ˆåŠŸèƒ½
        print("\n3. çµ±è¨ˆè³‡è¨Š:")
        stats = evaluator.get_evaluation_statistics()
        print(f"  ç¸½è©•ä¼°æ¬¡æ•¸: {stats['total_evaluations']}")
        print(f"  å¹³å‡åˆ†æ•¸æŒ‡æ¨™: {list(stats.get('current_averages', {}).keys())}")
        
        print("\nâœ… è©•ä¼°å™¨æ¸¬è©¦é€šé")
        return True
        
    except Exception as e:
        print(f"âŒ è©•ä¼°å™¨æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_evaluator()