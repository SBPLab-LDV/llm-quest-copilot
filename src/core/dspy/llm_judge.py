"""
LLM-as-Judge å›æ‡‰å“è³ªè©•ä¼°å™¨

ä½¿ç”¨ LLM è©•ä¼°ç—…æ‚£å›æ‡‰æ˜¯å¦é©åˆè©²å°è©±æ–¹è§’è‰²èˆ‡æƒ…å¢ƒã€‚
é€™æ˜¯ self-annotation æ©Ÿåˆ¶çš„å»¶ä¼¸ï¼Œç”¨æ–¼å“è³ªç›£æ§ã€‚

ä¸»è¦è©•ä¼°ç¶­åº¦ï¼š
1. Speaker é©åˆ‡æ€§ï¼šå›æ‡‰æ˜¯å¦é‡å°è©²è§’è‰²çš„å°ˆæ¥­ç¯„ç–‡
2. Context é©åˆ‡æ€§ï¼šå›æ‡‰æ˜¯å¦ç¬¦åˆè©²æƒ…å¢ƒçš„æœŸæœ›
3. ç—…æ‚£è¦–è§’ï¼šå›æ‡‰æ˜¯å¦ä»¥ç¬¬ä¸€äººç¨±æè¿°æ„Ÿå—
"""

import json
import logging
import re
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class ResponseQualityJudge:
    """ä½¿ç”¨ LLM è©•ä¼°å›æ‡‰å“è³ª

    è©•ä¼°æ¨™æº–ï¼š
    - Speaker é©åˆ‡æ€§ï¼šå›æ‡‰æ˜¯å¦é‡å°è©² speaker çš„å°ˆæ¥­ç¯„ç–‡
    - Context é©åˆ‡æ€§ï¼šå›æ‡‰æ˜¯å¦ç¬¦åˆè©²æƒ…å¢ƒçš„æœŸæœ›
    - ç—…æ‚£è¦–è§’ï¼šå›æ‡‰æ˜¯å¦ä»¥ç¬¬ä¸€äººç¨±æè¿°æ„Ÿå—

    ä½¿ç”¨æ–¹å¼ï¼š
        judge = ResponseQualityJudge()
        result = judge.evaluate(
            user_input="å˜´å·´ç¾åœ¨å¼µå¾—æ€éº¼æ¨£ï¼Ÿ",
            responses=["æ¯”è¼ƒé–‹äº†", "é‚„æ˜¯æœ‰é»ç·Š"],
            inferred_speaker="ç‰©ç†æ²»ç™‚å¸«",
            dialogue_context="ç‰©ç†æ²»ç™‚"
        )
    """

    JUDGE_PROMPT = """ä½ æ˜¯ä¸€ä½å°è©±å“è³ªè©•å¯©ã€‚è«‹è©•ä¼°ä»¥ä¸‹ç—…æ‚£å›æ‡‰æ˜¯å¦é©åˆè©²å°è©±æƒ…å¢ƒã€‚

å°è©±æ–¹è§’è‰²ï¼š{inferred_speaker}
å°è©±æƒ…å¢ƒï¼š{dialogue_context}
å°è©±æ–¹å•é¡Œï¼š{user_input}

ç—…æ‚£å›æ‡‰é¸é …ï¼š
{responses}

è«‹è©•ä¼°æ¯å€‹å›æ‡‰çš„é©åˆ‡æ€§ï¼ˆ1-5 åˆ†ï¼‰ï¼Œä¸¦èªªæ˜åŸå› ï¼š

è©•ä¼°æ¨™æº–ï¼š
1. Speaker é©åˆ‡æ€§ï¼šå›æ‡‰æ˜¯å¦é‡å°è©²è§’è‰²çš„å°ˆæ¥­ç¯„ç–‡ï¼Ÿ
   - ç‰©ç†æ²»ç™‚å¸«ï¼šæ‡‰é—œæ³¨é‹å‹•ã€å¼µå˜´ã€è‚Œè‚‰ã€å¾©å¥å‹•ä½œ
   - ç‡Ÿé¤Šå¸«ï¼šæ‡‰é—œæ³¨é£²é£Ÿã€ç‡Ÿé¤Šã€é«”é‡ã€é£Ÿæ…¾
   - è­·ç†å¸«ï¼šæ‡‰é—œæ³¨æ—¥å¸¸ç…§è­·ã€ç”Ÿå‘½å¾µè±¡ã€åŸºæœ¬éœ€æ±‚
   - é†«å¸«ï¼šæ‡‰é—œæ³¨æ²»ç™‚ã€è¨ºæ–·ã€æª¢æŸ¥ã€ç—…æƒ…
   - ç…§é¡§è€…ï¼šæ‡‰é—œæ³¨æƒ…ç·’ã€æ—¥å¸¸éœ€æ±‚ã€å®¶åº­é—œä¿‚

2. Context é©åˆ‡æ€§ï¼šå›æ‡‰æ˜¯å¦ç¬¦åˆè©²æƒ…å¢ƒçš„æœŸæœ›ï¼Ÿ

3. ç—…æ‚£è¦–è§’ï¼šå›æ‡‰æ˜¯å¦ä»¥ç¬¬ä¸€äººç¨±æè¿°æ„Ÿå—ï¼ˆè€Œéç¬¬ä¸‰äººç¨±æˆ–é†«ç™‚å°ˆæ¥­è¦–è§’ï¼‰ï¼Ÿ

è«‹è¼¸å‡º JSON æ ¼å¼ï¼š
{{
  "speaker_scores": [åˆ†æ•¸1, åˆ†æ•¸2, åˆ†æ•¸3, åˆ†æ•¸4],
  "context_scores": [åˆ†æ•¸1, åˆ†æ•¸2, åˆ†æ•¸3, åˆ†æ•¸4],
  "overall_score": æ•´é«”å¹³å‡åˆ†(1-5),
  "pass": trueæˆ–false (æ•´é«” >= 3.5 ç‚º pass),
  "issues": ["å•é¡Œ1", "å•é¡Œ2"],
  "reasoning": "ç°¡çŸ­è©•ä¼°èªªæ˜ï¼ˆ50å­—å…§ï¼‰"
}}"""

    # è§’è‰²å°ˆå±¬é—œéµè©ï¼ˆç”¨æ–¼å¿«é€Ÿé—œéµè©æª¢æŸ¥ï¼‰
    SPEAKER_KEYWORDS: Dict[str, List[str]] = {
        "ç‰©ç†æ²»ç™‚å¸«": ["å¼µå˜´", "å¼µå£", "ç·Š", "é¬†", "é‹å‹•", "ç·´ç¿’", "è‚Œè‚‰", "æ´»å‹•", "å¾©å¥"],
        "ç‡Ÿé¤Šå¸«": ["åƒ", "é£Ÿæ…¾", "é…æ–¹", "ç‡Ÿé¤Š", "é£²é£Ÿ", "é«”é‡", "ç†±é‡", "è£œå……"],
        "è­·ç†å¸«": ["èˆ’æœ", "ç—›", "è¡€å£“", "é«”æº«", "æ›è—¥", "å‚·å£", "å¥½"],
        "é†«å¸«": ["æ²»ç™‚", "æ‰‹è¡“", "è¨ºæ–·", "æª¢æŸ¥", "å ±å‘Š", "è¿½è¹¤", "æŒ‡æ•¸"],
        "ç…§é¡§è€…": ["å¥½é»", "ä¼‘æ¯", "ç´¯", "æƒ³", "éœ€è¦", "æ“”å¿ƒ", "ç²¾ç¥"],
    }

    # è§’è‰²ç¦æ­¢é—œéµè©ï¼ˆå‡ºç¾å‰‡æ‰£åˆ†ï¼‰
    SPEAKER_FORBIDDEN: Dict[str, List[str]] = {
        "ç‰©ç†æ²»ç™‚å¸«": ["åƒé£¯", "è—¥ç‰©", "è¡€å£“", "æª¢æŸ¥å ±å‘Š"],
        "ç‡Ÿé¤Šå¸«": ["å¼µå˜´", "å¾©å¥å‹•ä½œ", "å‚·å£"],
        "è­·ç†å¸«": ["æ²»ç™‚æ–¹æ¡ˆæ±ºå®š", "å¾©å¥è¨ˆç•«"],
        "é†«å¸«": ["æ›è—¥ç¨‹åº", "æ—¥å¸¸ç‘£äº‹"],
        "ç…§é¡§è€…": ["è¨ºæ–·", "æª¢æŸ¥å ±å‘Š", "æ²»ç™‚æ–¹æ¡ˆ"],
    }

    def __init__(self, lm=None):
        """åˆå§‹åŒ– Judge

        Args:
            lm: DSPy LM å¯¦ä¾‹ã€‚å¦‚æœç‚º Noneï¼Œå°‡ä½¿ç”¨ dspy.settings.lm
        """
        self.lm = lm

    def _get_lm(self):
        """å–å¾— LM å¯¦ä¾‹"""
        if self.lm is not None:
            return self.lm
        try:
            import dspy
            return dspy.settings.lm
        except Exception as e:
            logger.warning(f"ç„¡æ³•å–å¾— DSPy LM: {e}")
            return None

    def evaluate(
        self,
        user_input: str,
        responses: List[str],
        inferred_speaker: str,
        dialogue_context: str,
        use_llm: bool = True,
    ) -> Dict[str, Any]:
        """è©•ä¼°å›æ‡‰å“è³ª

        Args:
            user_input: å°è©±æ–¹çš„å•é¡Œ
            responses: ç—…æ‚£å›æ‡‰é¸é …åˆ—è¡¨
            inferred_speaker: æ¨ç†å‡ºçš„å°è©±æ–¹è§’è‰²
            dialogue_context: å°è©±æƒ…å¢ƒ
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM è©•ä¼°ï¼ˆFalse å‰‡åªç”¨é—œéµè©ï¼‰

        Returns:
            è©•ä¼°çµæœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - speaker_scores: æ¯å€‹å›æ‡‰çš„ Speaker é©åˆ‡æ€§åˆ†æ•¸
            - context_scores: æ¯å€‹å›æ‡‰çš„ Context é©åˆ‡æ€§åˆ†æ•¸
            - overall_score: æ•´é«”å¹³å‡åˆ†
            - pass: æ˜¯å¦é€šéï¼ˆ>= 3.5ï¼‰
            - issues: ç™¼ç¾çš„å•é¡Œ
            - reasoning: è©•ä¼°èªªæ˜
        """
        # é—œéµè©å¿«é€Ÿæª¢æŸ¥ï¼ˆç¸½æ˜¯åŸ·è¡Œï¼‰
        keyword_result = self._keyword_check(responses, inferred_speaker)

        if not use_llm:
            return keyword_result

        # LLM è©•ä¼°
        lm = self._get_lm()
        if lm is None:
            logger.warning("LLM ä¸å¯ç”¨ï¼Œä½¿ç”¨é—œéµè©è©•ä¼°")
            return keyword_result

        try:
            return self._llm_evaluate(
                user_input, responses, inferred_speaker, dialogue_context
            )
        except Exception as e:
            logger.error(f"LLM è©•ä¼°å¤±æ•—: {e}")
            return keyword_result

    def _keyword_check(
        self, responses: List[str], inferred_speaker: str
    ) -> Dict[str, Any]:
        """ä½¿ç”¨é—œéµè©é€²è¡Œå¿«é€Ÿå“è³ªæª¢æŸ¥

        Args:
            responses: å›æ‡‰åˆ—è¡¨
            inferred_speaker: å°è©±æ–¹è§’è‰²

        Returns:
            è©•ä¼°çµæœ
        """
        target_keywords = self.SPEAKER_KEYWORDS.get(inferred_speaker, [])
        forbidden_keywords = self.SPEAKER_FORBIDDEN.get(inferred_speaker, [])

        responses_text = " ".join(responses)
        issues = []

        # è¨ˆç®—æ­£å‘åŒ¹é…
        positive_hits = sum(1 for kw in target_keywords if kw in responses_text)

        # è¨ˆç®—è² å‘åŒ¹é…
        forbidden_hits = []
        for kw in forbidden_keywords:
            if kw in responses_text:
                forbidden_hits.append(kw)
                issues.append(f"å‡ºç¾ä¸é©åˆè©å½™ï¼š{kw}")

        # è¨ˆç®—åˆ†æ•¸
        if target_keywords:
            base_score = (positive_hits / len(target_keywords)) * 5
        else:
            base_score = 3.0  # ç„¡é—œéµè©æ™‚çµ¦ä¸­ç­‰åˆ†æ•¸

        penalty = len(forbidden_hits) * 0.5
        overall_score = max(1.0, min(5.0, base_score - penalty))

        return {
            "speaker_scores": [overall_score] * len(responses),
            "context_scores": [overall_score] * len(responses),
            "overall_score": round(overall_score, 2),
            "pass": overall_score >= 3.5,
            "issues": issues,
            "reasoning": f"é—œéµè©æª¢æŸ¥ï¼šæ­£å‘åŒ¹é… {positive_hits}/{len(target_keywords)}ï¼Œç¦æ­¢è© {len(forbidden_hits)}",
            "method": "keyword",
        }

    def _llm_evaluate(
        self,
        user_input: str,
        responses: List[str],
        inferred_speaker: str,
        dialogue_context: str,
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ LLM é€²è¡Œæ·±åº¦è©•ä¼°

        Args:
            user_input: å°è©±æ–¹çš„å•é¡Œ
            responses: å›æ‡‰åˆ—è¡¨
            inferred_speaker: å°è©±æ–¹è§’è‰²
            dialogue_context: å°è©±æƒ…å¢ƒ

        Returns:
            LLM è©•ä¼°çµæœ
        """
        prompt = self.JUDGE_PROMPT.format(
            inferred_speaker=inferred_speaker,
            dialogue_context=dialogue_context,
            user_input=user_input,
            responses="\n".join(f"{i+1}. {r}" for i, r in enumerate(responses)),
        )

        lm = self._get_lm()
        result = lm(prompt)

        # è§£æå›æ‡‰
        if hasattr(result, "text"):
            text = result.text
        elif hasattr(result, "content"):
            text = result.content
        elif isinstance(result, list) and len(result) > 0:
            text = str(result[0])
        else:
            text = str(result)

        # å˜—è©¦æå– JSON
        try:
            # å˜—è©¦ç›´æ¥è§£æ
            parsed = json.loads(text)
        except json.JSONDecodeError:
            # å˜—è©¦å¾æ–‡æœ¬ä¸­æå– JSON
            json_match = re.search(r"\{[\s\S]*\}", text)
            if json_match:
                try:
                    parsed = json.loads(json_match.group())
                except json.JSONDecodeError:
                    parsed = None
            else:
                parsed = None

        if parsed is None:
            return {
                "error": "ç„¡æ³•è§£æ LLM å›æ‡‰",
                "raw": text[:500],
                "method": "llm_failed",
            }

        parsed["method"] = "llm"
        return parsed

    def batch_evaluate(
        self, test_cases: List[Dict[str, Any]], use_llm: bool = True
    ) -> Dict[str, Any]:
        """æ‰¹é‡è©•ä¼°å¤šå€‹æ¸¬è©¦æ¡ˆä¾‹

        Args:
            test_cases: æ¸¬è©¦æ¡ˆä¾‹åˆ—è¡¨ï¼Œæ¯å€‹åŒ…å«ï¼š
                - question: å°è©±æ–¹å•é¡Œ
                - responses: å›æ‡‰åˆ—è¡¨
                - inferred_speaker: æ¨ç†çš„è§’è‰²
                - dialogue_context: å°è©±æƒ…å¢ƒ
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM è©•ä¼°

        Returns:
            æ‰¹é‡è©•ä¼°çµæœï¼ŒåŒ…å«ï¼š
            - total: ç¸½æ¡ˆä¾‹æ•¸
            - passed: é€šéæ•¸
            - pass_rate: é€šéç‡
            - average_score: å¹³å‡åˆ†
            - details: æ¯å€‹æ¡ˆä¾‹çš„è©³ç´°çµæœ
        """
        results = []
        total_score = 0.0

        for case in test_cases:
            result = self.evaluate(
                user_input=case["question"],
                responses=case.get("responses", []),
                inferred_speaker=case.get("inferred_speaker", ""),
                dialogue_context=case.get("dialogue_context", ""),
                use_llm=use_llm,
            )
            results.append({**case, "evaluation": result})

            if "overall_score" in result:
                total_score += result["overall_score"]

        # è¨ˆç®—çµ±è¨ˆ
        passed = sum(
            1 for r in results if r.get("evaluation", {}).get("pass", False)
        )
        avg_score = total_score / len(results) if results else 0

        return {
            "total": len(results),
            "passed": passed,
            "pass_rate": passed / len(results) if results else 0,
            "average_score": round(avg_score, 2),
            "details": results,
        }


def quick_evaluate(
    user_input: str,
    responses: List[str],
    inferred_speaker: str,
    dialogue_context: str = "",
) -> Dict[str, Any]:
    """å¿«é€Ÿè©•ä¼°å–®å€‹å›æ‡‰ï¼ˆä¾¿æ·å‡½æ•¸ï¼‰

    Args:
        user_input: å°è©±æ–¹å•é¡Œ
        responses: å›æ‡‰åˆ—è¡¨
        inferred_speaker: å°è©±æ–¹è§’è‰²
        dialogue_context: å°è©±æƒ…å¢ƒ

    Returns:
        è©•ä¼°çµæœ
    """
    judge = ResponseQualityJudge()
    return judge.evaluate(
        user_input=user_input,
        responses=responses,
        inferred_speaker=inferred_speaker,
        dialogue_context=dialogue_context,
        use_llm=False,  # å¿«é€Ÿæ¨¡å¼åªç”¨é—œéµè©
    )


# æ¸¬è©¦å‡½æ•¸
def test_llm_judge():
    """æ¸¬è©¦ LLM Judge åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦ ResponseQualityJudge...")

    judge = ResponseQualityJudge()

    # æ¸¬è©¦æ¡ˆä¾‹ 1ï¼šç‰©ç†æ²»ç™‚å¸«
    result1 = judge.evaluate(
        user_input="å˜´å·´ç¾åœ¨å¼µå¾—æ€éº¼æ¨£ï¼Ÿ",
        responses=["æ¯”è¼ƒé–‹äº†", "é‚„æ˜¯æœ‰é»ç·Šç¹ƒ", "å¥½åƒæœ‰é€²æ­¥"],
        inferred_speaker="ç‰©ç†æ²»ç™‚å¸«",
        dialogue_context="ç‰©ç†æ²»ç™‚",
        use_llm=False,
    )
    print(f"\nç‰©ç†æ²»ç™‚å¸«æ¡ˆä¾‹ï¼š")
    print(f"  åˆ†æ•¸: {result1.get('overall_score')}")
    print(f"  é€šé: {result1.get('pass')}")
    print(f"  èªªæ˜: {result1.get('reasoning')}")

    # æ¸¬è©¦æ¡ˆä¾‹ 2ï¼šç‡Ÿé¤Šå¸«
    result2 = judge.evaluate(
        user_input="ç‡Ÿé¤Šå“æœ‰åœ¨åƒå—ï¼Ÿ",
        responses=["æœ‰åœ¨åƒé…æ–¹å¥¶", "æ²’ä»€éº¼é£Ÿæ…¾"],
        inferred_speaker="ç‡Ÿé¤Šå¸«",
        dialogue_context="ç‡Ÿé¤Šè«®è©¢",
        use_llm=False,
    )
    print(f"\nç‡Ÿé¤Šå¸«æ¡ˆä¾‹ï¼š")
    print(f"  åˆ†æ•¸: {result2.get('overall_score')}")
    print(f"  é€šé: {result2.get('pass')}")

    # æ¸¬è©¦æ‰¹é‡è©•ä¼°
    test_cases = [
        {
            "question": "å˜´å·´å¼µå¾—æ€æ¨£ï¼Ÿ",
            "responses": ["æ¯”è¼ƒé–‹äº†"],
            "inferred_speaker": "ç‰©ç†æ²»ç™‚å¸«",
            "dialogue_context": "ç‰©ç†æ²»ç™‚",
        },
        {
            "question": "ç‡Ÿé¤Šå“æœ‰åƒå—ï¼Ÿ",
            "responses": ["æœ‰åƒé…æ–¹å¥¶"],
            "inferred_speaker": "ç‡Ÿé¤Šå¸«",
            "dialogue_context": "ç‡Ÿé¤Šè«®è©¢",
        },
    ]
    batch_result = judge.batch_evaluate(test_cases, use_llm=False)
    print(f"\næ‰¹é‡è©•ä¼°çµæœï¼š")
    print(f"  é€šéç‡: {batch_result['pass_rate']:.1%}")
    print(f"  å¹³å‡åˆ†: {batch_result['average_score']}")

    print("\nâœ… LLM Judge æ¸¬è©¦å®Œæˆ")
    return True


if __name__ == "__main__":
    test_llm_judge()
