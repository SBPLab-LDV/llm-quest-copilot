"""
DSPy é€€åŒ–ç›£æ§ç³»çµ± (DSPy Degradation Monitor)

é€™å€‹æ¨¡çµ„å¯¦ç¾äº†å¯¦æ™‚å°è©±å“è³ªç›£æ§å’Œé€€åŒ–é è­¦ç³»çµ±ï¼Œç”¨æ–¼æª¢æ¸¬å’Œé é˜² DSPy å°è©±æ¨¡çµ„çš„å“è³ªé€€åŒ–ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. å¯¦æ™‚å“è³ªè©•ä¼°ï¼šè©•ä¼°å°è©±å›æ‡‰çš„å“è³ªå’Œä¸€è‡´æ€§
2. é€€åŒ–æ¨¡å¼è­˜åˆ¥ï¼šæª¢æ¸¬è‡ªæˆ‘ä»‹ç´¹æ¨¡å¼ã€é€šç”¨å›æ‡‰ç­‰é€€åŒ–æŒ‡æ¨™
3. é è­¦ç³»çµ±ï¼šåŸºæ–¼å“è³ªé–¾å€¼è§¸ç™¼é€€åŒ–é è­¦
4. å“è³ªæŒ‡æ¨™è¨ˆç®—ï¼šè¨ˆç®—è§’è‰²ä¸€è‡´æ€§ã€ç›¸é—œæ€§ç­‰é—œéµæŒ‡æ¨™
5. æ­·å²è¶¨å‹¢åˆ†æï¼šè¿½è¹¤å“è³ªè®ŠåŒ–è¶¨å‹¢

Author: DSPy Analysis Team
Date: 2025-09-12
Version: 1.0.0
"""

import logging
import re
import statistics
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
from dataclasses import dataclass
from enum import Enum


class DegradationRisk(Enum):
    """é€€åŒ–é¢¨éšªç­‰ç´š"""
    LOW = "low"
    MEDIUM = "medium"  
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class QualityMetrics:
    """å“è³ªæŒ‡æ¨™æ•¸æ“šé¡"""
    character_consistency_score: float
    response_relevance_score: float
    context_appropriateness_score: float
    self_introduction_detected: bool
    generic_response_detected: bool
    response_count_normal: bool
    reasoning_quality_score: float
    overall_quality_score: float
    risk_level: DegradationRisk


@dataclass
class DegradationPattern:
    """é€€åŒ–æ¨¡å¼æ•¸æ“šé¡"""
    pattern_name: str
    detection_confidence: float
    indicators: List[str]
    risk_contribution: float


class DegradationMonitor:
    """
    DSPy é€€åŒ–ç›£æ§ç³»çµ±ä¸»é¡
    
    è² è²¬ç›£æ§å°è©±å“è³ªã€æª¢æ¸¬é€€åŒ–æ¨¡å¼ã€æä¾›é è­¦æ©Ÿåˆ¶
    """
    
    def __init__(self, enable_logging: bool = True):
        """
        åˆå§‹åŒ–é€€åŒ–ç›£æ§ç³»çµ±
        
        Args:
            enable_logging: æ˜¯å¦å•Ÿç”¨è©³ç´°æ—¥èªŒè¨˜éŒ„
        """
        self.logger = logging.getLogger(__name__)
        self.enable_logging = enable_logging
        
        # å“è³ªæ­·å²è¨˜éŒ„
        self.quality_history: List[QualityMetrics] = []
        self.degradation_events: List[Dict[str, Any]] = []
        
        # é€€åŒ–æª¢æ¸¬é–¾å€¼
        self.thresholds = {
            'character_consistency': 0.7,
            'response_relevance': 0.6,
            'context_appropriateness': 0.6,
            'reasoning_quality': 0.5,
            'overall_quality': 0.6,
            'critical_round_start': 3  # ç¬¬3è¼ªé–‹å§‹é€²å…¥é—œéµç›£æ§æœŸ
        }
        
        # é€€åŒ–æ¨¡å¼æª¢æ¸¬è¦å‰‡
        self._init_degradation_patterns()
        
        if self.enable_logging:
            self.logger.info("ğŸ” DSPy é€€åŒ–ç›£æ§ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    def _init_degradation_patterns(self):
        """åˆå§‹åŒ–é€€åŒ–æ¨¡å¼æª¢æ¸¬è¦å‰‡"""
        self.degradation_patterns = {
            'self_introduction': {
                'patterns': [
                    r'æ‚¨å¥½ï¼Œæˆ‘æ˜¯\w+',
                    r'æˆ‘æ˜¯\w+ï¼Œ\w+ç—…æ‚£',
                    r'æˆ‘æ˜¯.*Patient_\d+',
                    r'æ‚¨å¥½.*æˆ‘æ˜¯.*ç—…æ‚£'
                ],
                'risk_weight': 0.9,
                'description': 'è‡ªæˆ‘ä»‹ç´¹æ¨¡å¼'
            },
            'generic_response': {
                'patterns': [
                    r'æˆ‘å¯èƒ½æ²’æœ‰å®Œå…¨ç†è§£æ‚¨çš„å•é¡Œ',
                    r'èƒ½è«‹æ‚¨æ›å€‹æ–¹å¼èªªæ˜å—',
                    r'æ‚¨éœ€è¦ä»€éº¼å¹«åŠ©å—',
                    r'æŠ±æ­‰.*æ²’æ³•.*ç†è§£',
                    r'æˆ‘ç†è§£æ‚¨æƒ³äº†è§£'
                ],
                'risk_weight': 0.8,
                'description': 'é€šç”¨å›æ‡‰æ¨¡å¼'
            },
            'context_confusion': {
                'patterns': [
                    r'ä¸€èˆ¬å•è¨ºå°è©±',
                    r'ä¸ç¢ºå®š.*æƒ…å¢ƒ',
                    r'æ··äº‚.*ç‹€æ…‹'
                ],
                'risk_weight': 0.7,
                'description': 'æƒ…å¢ƒæ··äº‚æ¨¡å¼'
            },
            'role_inconsistency': {
                'patterns': [
                    r'åŠ©æ‰‹.*æ¨¡å¼',
                    r'AI.*ç³»çµ±',
                    r'å¿˜è¨˜.*è§’è‰²'
                ],
                'risk_weight': 0.8,
                'description': 'è§’è‰²ä¸ä¸€è‡´æ¨¡å¼'
            }
        }
    
    def assess_dialogue_quality(self, response_data: Dict[str, Any], 
                              round_number: int, 
                              conversation_history: List[Dict] = None,
                              character_config: Dict[str, Any] = None) -> QualityMetrics:
        """
        è©•ä¼°å°è©±å“è³ª
        
        Args:
            response_data: å°è©±å›æ‡‰æ•¸æ“š
            round_number: ç•¶å‰å°è©±è¼ªæ¬¡
            conversation_history: å°è©±æ­·å²
            character_config: è§’è‰²é…ç½®
            
        Returns:
            QualityMetrics: å“è³ªè©•ä¼°çµæœ
        """
        if self.enable_logging:
            self.logger.info(f"ğŸ” DEGRADATION MONITOR: é–‹å§‹è©•ä¼°ç¬¬ {round_number} è¼ªå°è©±å“è³ª")
        
        try:
            # æå–å›æ‡‰å…§å®¹
            responses = response_data.get('responses', [])
            dialogue_context = response_data.get('dialogue_context', '')
            reasoning = response_data.get('reasoning', '')
            state = response_data.get('state', 'NORMAL')
            
            # è¨ˆç®—å„é …å“è³ªæŒ‡æ¨™
            character_consistency = self._calculate_character_consistency(
                responses, character_config, reasoning
            )
            
            response_relevance = self._calculate_response_relevance(
                responses, dialogue_context, conversation_history
            )
            
            context_appropriateness = self._calculate_context_appropriateness(
                dialogue_context, round_number
            )
            
            reasoning_quality = self._calculate_reasoning_quality(reasoning)
            
            # æª¢æ¸¬é€€åŒ–æ¨¡å¼
            self_intro_detected = self._detect_self_introduction(responses)
            generic_detected = self._detect_generic_response(responses)
            count_normal = len(responses) >= 3  # æ­£å¸¸æ‡‰è©²æœ‰3-5å€‹å›æ‡‰é¸é …
            
            # è¨ˆç®—ç¸½é«”å“è³ªåˆ†æ•¸
            overall_quality = self._calculate_overall_quality_score(
                character_consistency, response_relevance, context_appropriateness,
                reasoning_quality, self_intro_detected, generic_detected, count_normal
            )
            
            # è©•ä¼°é¢¨éšªç­‰ç´š
            risk_level = self._assess_risk_level(
                overall_quality, round_number, self_intro_detected, generic_detected
            )
            
            # å»ºç«‹å“è³ªæŒ‡æ¨™å°è±¡
            metrics = QualityMetrics(
                character_consistency_score=character_consistency,
                response_relevance_score=response_relevance,
                context_appropriateness_score=context_appropriateness,
                self_introduction_detected=self_intro_detected,
                generic_response_detected=generic_detected,
                response_count_normal=count_normal,
                reasoning_quality_score=reasoning_quality,
                overall_quality_score=overall_quality,
                risk_level=risk_level
            )
            
            # è¨˜éŒ„å“è³ªæ­·å²
            self.quality_history.append(metrics)
            
            # è¨˜éŒ„è©³ç´°æ—¥èªŒ
            if self.enable_logging:
                self._log_quality_assessment(metrics, round_number)
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦è§¸ç™¼é è­¦
            self._check_degradation_warning(metrics, round_number)
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"âŒ DEGRADATION MONITOR: å“è³ªè©•ä¼°éŒ¯èª¤: {str(e)}")
            # è¿”å›ä½å“è³ªæŒ‡æ¨™ä½œç‚ºå®‰å…¨æªæ–½
            return QualityMetrics(
                character_consistency_score=0.0,
                response_relevance_score=0.0,
                context_appropriateness_score=0.0,
                self_introduction_detected=True,
                generic_response_detected=True,
                response_count_normal=False,
                reasoning_quality_score=0.0,
                overall_quality_score=0.0,
                risk_level=DegradationRisk.CRITICAL
            )
    
    def _calculate_character_consistency(self, responses: List[str], 
                                       character_config: Dict[str, Any], 
                                       reasoning: str) -> float:
        """è¨ˆç®—è§’è‰²ä¸€è‡´æ€§åˆ†æ•¸"""
        if not responses or not character_config:
            return 0.5
        
        try:
            score = 0.8  # åŸºç¤åˆ†æ•¸
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«è§’è‰²åç¨±ä½†éè‡ªæˆ‘ä»‹ç´¹å½¢å¼
            character_name = character_config.get('name', '')
            if character_name:
                for response in responses:
                    if character_name in response:
                        # å¦‚æœæ˜¯è‡ªæˆ‘ä»‹ç´¹å½¢å¼ï¼Œæ‰£åˆ†
                        if re.search(r'æˆ‘æ˜¯.*' + character_name, response):
                            score -= 0.3
                        else:
                            score += 0.1
            
            # æª¢æŸ¥æ¨ç†ä¸­çš„è§’è‰²æ„è­˜
            if reasoning and 'è§’è‰²' in reasoning:
                score += 0.1
            
            # æª¢æŸ¥æ˜¯å¦ç¶­æŒç—…æ‚£èº«ä»½
            patient_indicators = ['ç—…æ‚£', 'æ‰‹è¡“', 'æ²»ç™‚', 'é†«å¸«', 'æª¢æŸ¥']
            for response in responses:
                if any(indicator in response for indicator in patient_indicators):
                    score += 0.05
            
            return min(max(score, 0.0), 1.0)
            
        except Exception as e:
            self.logger.error(f"è§’è‰²ä¸€è‡´æ€§è¨ˆç®—éŒ¯èª¤: {str(e)}")
            return 0.5
    
    def _calculate_response_relevance(self, responses: List[str], 
                                    dialogue_context: str,
                                    conversation_history: List[Dict] = None) -> float:
        """è¨ˆç®—å›æ‡‰ç›¸é—œæ€§åˆ†æ•¸"""
        if not responses:
            return 0.0
        
        try:
            score = 0.7  # åŸºç¤åˆ†æ•¸
            
            # æª¢æŸ¥å°è©±æƒ…å¢ƒç›¸é—œæ€§
            if dialogue_context:
                context_keywords = {
                    'é†«å¸«æŸ¥æˆ¿': ['æŸ¥æˆ¿', 'é†«å¸«', 'èº«é«”', 'ç‹€æ³'],
                    'èº«é«”è©•ä¼°': ['è©•ä¼°', 'ç—‡ç‹€', 'æ„Ÿè¦º', 'ä¸èˆ’æœ'],
                    'æª¢æŸ¥ç›¸é—œ': ['æª¢æŸ¥', 'å®‰æ’', 'æº–å‚™', 'é…åˆ']
                }
                
                for context_type, keywords in context_keywords.items():
                    if context_type in dialogue_context:
                        for response in responses:
                            if any(keyword in response for keyword in keywords):
                                score += 0.1
                        break
            
            # æª¢æŸ¥æ˜¯å¦æœ‰å…·é«”é†«ç™‚ç›¸é—œå›æ‡‰
            medical_terms = ['æ‰‹è¡“', 'å‚·å£', 'è…«è„¹', 'ç–¼ç—›', 'æ²»ç™‚', 'åº·å¾©']
            for response in responses:
                if any(term in response for term in medical_terms):
                    score += 0.05
            
            return min(max(score, 0.0), 1.0)
            
        except Exception as e:
            self.logger.error(f"å›æ‡‰ç›¸é—œæ€§è¨ˆç®—éŒ¯èª¤: {str(e)}")
            return 0.5
    
    def _calculate_context_appropriateness(self, dialogue_context: str, round_number: int) -> float:
        """è¨ˆç®—æƒ…å¢ƒé©åˆ‡æ€§åˆ†æ•¸"""
        try:
            score = 0.8  # åŸºç¤åˆ†æ•¸
            
            # æª¢æŸ¥æ˜¯å¦é€€åŒ–ç‚ºé€šç”¨æƒ…å¢ƒ
            if 'ä¸€èˆ¬å•è¨ºå°è©±' in dialogue_context:
                score -= 0.4
            
            # æª¢æŸ¥æƒ…å¢ƒæ˜¯å¦èˆ‡è¼ªæ¬¡ç›¸ç¬¦
            expected_contexts = {
                1: ['é†«å¸«æŸ¥æˆ¿', 'åˆæ¬¡æ¥è§¸'],
                2: ['èº«é«”è©•ä¼°', 'ç—‡ç‹€è©¢å•'],
                3: ['èº«é«”è©•ä¼°', 'è©³ç´°è©•ä¼°'],
                4: ['ç—‡ç‹€è©•ä¼°', 'é†«å¸«æŸ¥æˆ¿'],
                5: ['æª¢æŸ¥ç›¸é—œ', 'æ²»ç™‚å®‰æ’']
            }
            
            if round_number in expected_contexts:
                expected = expected_contexts[round_number]
                if any(context in dialogue_context for context in expected):
                    score += 0.1
                else:
                    score -= 0.2
            
            return min(max(score, 0.0), 1.0)
            
        except Exception as e:
            self.logger.error(f"æƒ…å¢ƒé©åˆ‡æ€§è¨ˆç®—éŒ¯èª¤: {str(e)}")
            return 0.5
    
    def _calculate_reasoning_quality(self, reasoning: str) -> float:
        """è¨ˆç®—æ¨ç†å“è³ªåˆ†æ•¸"""
        if not reasoning:
            return 0.3
        
        try:
            score = 0.5  # åŸºç¤åˆ†æ•¸
            
            # æª¢æŸ¥æ¨ç†é•·åº¦ï¼ˆéçŸ­å¯èƒ½æ˜¯é€€åŒ–ï¼‰
            if len(reasoning) > 100:
                score += 0.2
            elif len(reasoning) < 30:
                score -= 0.3
            
            # æª¢æŸ¥æ¨ç†å…§å®¹å“è³ª
            quality_indicators = [
                'è§’è‰²', 'ç—…æ‚£', 'é†«ç™‚', 'æƒ…å¢ƒ', 'ç—‡ç‹€', 
                'è©•ä¼°', 'åˆ†æ', 'è€ƒæ…®', 'æ ¹æ“š'
            ]
            
            for indicator in quality_indicators:
                if indicator in reasoning:
                    score += 0.05
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ç³»çµ±æ€§æ€è€ƒ
            reasoning_patterns = [
                r'è€ƒæ…®åˆ°.*æƒ…æ³',
                r'åŸºæ–¼.*åˆ†æ',
                r'å¾.*è§’åº¦'
            ]
            
            for pattern in reasoning_patterns:
                if re.search(pattern, reasoning):
                    score += 0.1
            
            return min(max(score, 0.0), 1.0)
            
        except Exception as e:
            self.logger.error(f"æ¨ç†å“è³ªè¨ˆç®—éŒ¯èª¤: {str(e)}")
            return 0.3
    
    def _detect_self_introduction(self, responses: List[str]) -> bool:
        """æª¢æ¸¬è‡ªæˆ‘ä»‹ç´¹æ¨¡å¼"""
        if not responses:
            return False
        
        patterns = self.degradation_patterns['self_introduction']['patterns']
        
        for response in responses:
            for pattern in patterns:
                if re.search(pattern, response):
                    return True
        
        return False
    
    def _detect_generic_response(self, responses: List[str]) -> bool:
        """æª¢æ¸¬é€šç”¨å›æ‡‰æ¨¡å¼"""
        if not responses:
            return False
        
        patterns = self.degradation_patterns['generic_response']['patterns']
        
        for response in responses:
            for pattern in patterns:
                if re.search(pattern, response):
                    return True
        
        return False
    
    def _calculate_overall_quality_score(self, character_consistency: float,
                                       response_relevance: float,
                                       context_appropriateness: float,
                                       reasoning_quality: float,
                                       self_intro_detected: bool,
                                       generic_detected: bool,
                                       count_normal: bool) -> float:
        """è¨ˆç®—ç¸½é«”å“è³ªåˆ†æ•¸"""
        try:
            # åŸºç¤å“è³ªåˆ†æ•¸ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰
            base_score = (
                character_consistency * 0.3 +
                response_relevance * 0.25 +
                context_appropriateness * 0.25 +
                reasoning_quality * 0.2
            )
            
            # é€€åŒ–æ¨¡å¼æ‡²ç½°
            if self_intro_detected:
                base_score -= 0.4
            
            if generic_detected:
                base_score -= 0.3
            
            if not count_normal:
                base_score -= 0.2
            
            return min(max(base_score, 0.0), 1.0)
            
        except Exception as e:
            self.logger.error(f"ç¸½é«”å“è³ªåˆ†æ•¸è¨ˆç®—éŒ¯èª¤: {str(e)}")
            return 0.0
    
    def _assess_risk_level(self, overall_quality: float, 
                          round_number: int,
                          self_intro_detected: bool,
                          generic_detected: bool) -> DegradationRisk:
        """è©•ä¼°é€€åŒ–é¢¨éšªç­‰ç´š"""
        try:
            # é—œéµè¼ªæ¬¡ï¼ˆ3-5è¼ªï¼‰æé«˜è­¦æˆ’
            is_critical_round = round_number >= self.thresholds['critical_round_start']
            
            # åš´é‡é€€åŒ–æŒ‡æ¨™
            if self_intro_detected or generic_detected:
                return DegradationRisk.CRITICAL
            
            # æ ¹æ“šå“è³ªåˆ†æ•¸åˆ¤æ–·
            if overall_quality >= 0.8:
                return DegradationRisk.LOW
            elif overall_quality >= 0.6:
                return DegradationRisk.MEDIUM if is_critical_round else DegradationRisk.LOW
            elif overall_quality >= 0.4:
                return DegradationRisk.HIGH if is_critical_round else DegradationRisk.MEDIUM
            else:
                return DegradationRisk.CRITICAL
                
        except Exception as e:
            self.logger.error(f"é¢¨éšªç­‰ç´šè©•ä¼°éŒ¯èª¤: {str(e)}")
            return DegradationRisk.CRITICAL
    
    def _log_quality_assessment(self, metrics: QualityMetrics, round_number: int):
        """è¨˜éŒ„å“è³ªè©•ä¼°è©³ç´°æ—¥èªŒ"""
        self.logger.info(f"ğŸ“Š QUALITY ASSESSMENT - Round {round_number}")
        self.logger.info(f"  ğŸ¯ Overall Quality: {metrics.overall_quality_score:.3f}")
        self.logger.info(f"  ğŸ‘¤ Character Consistency: {metrics.character_consistency_score:.3f}")
        self.logger.info(f"  ğŸª Response Relevance: {metrics.response_relevance_score:.3f}")
        self.logger.info(f"  ğŸŒ Context Appropriateness: {metrics.context_appropriateness_score:.3f}")
        self.logger.info(f"  ğŸ§  Reasoning Quality: {metrics.reasoning_quality_score:.3f}")
        self.logger.info(f"  ğŸš¨ Self Introduction: {metrics.self_introduction_detected}")
        self.logger.info(f"  ğŸ¤– Generic Response: {metrics.generic_response_detected}")
        self.logger.info(f"  ğŸ“ Response Count Normal: {metrics.response_count_normal}")
        self.logger.info(f"  âš ï¸ Risk Level: {metrics.risk_level.value.upper()}")
    
    def _check_degradation_warning(self, metrics: QualityMetrics, round_number: int):
        """æª¢æŸ¥æ˜¯å¦éœ€è¦è§¸ç™¼é€€åŒ–é è­¦"""
        if metrics.risk_level in [DegradationRisk.HIGH, DegradationRisk.CRITICAL]:
            warning_event = {
                'timestamp': datetime.now().isoformat(),
                'round_number': round_number,
                'risk_level': metrics.risk_level.value,
                'quality_score': metrics.overall_quality_score,
                'degradation_indicators': []
            }
            
            if metrics.self_introduction_detected:
                warning_event['degradation_indicators'].append('self_introduction')
            
            if metrics.generic_response_detected:
                warning_event['degradation_indicators'].append('generic_response')
            
            if not metrics.response_count_normal:
                warning_event['degradation_indicators'].append('abnormal_response_count')
            
            self.degradation_events.append(warning_event)
            
            # ç™¼å‡ºé è­¦æ—¥èªŒ
            self.logger.warning(f"ğŸš¨ DEGRADATION WARNING - Round {round_number}")
            self.logger.warning(f"  âš ï¸ Risk Level: {metrics.risk_level.value.upper()}")
            self.logger.warning(f"  ğŸ“Š Quality Score: {metrics.overall_quality_score:.3f}")
            self.logger.warning(f"  ğŸ” Indicators: {warning_event['degradation_indicators']}")
    
    def get_quality_trend_analysis(self, window_size: int = 3) -> Dict[str, Any]:
        """ç²å–å“è³ªè¶¨å‹¢åˆ†æ"""
        if len(self.quality_history) < 2:
            return {
                'trend': 'insufficient_data',
                'average_quality': 0.0,
                'quality_variance': 0.0,
                'degradation_risk_trend': 'unknown'
            }
        
        try:
            recent_scores = [m.overall_quality_score for m in self.quality_history[-window_size:]]
            
            # è¨ˆç®—è¶‹å‹¢
            if len(recent_scores) >= 2:
                trend = 'improving' if recent_scores[-1] > recent_scores[-2] else 'declining'
            else:
                trend = 'stable'
            
            # è¨ˆç®—çµ±è¨ˆå€¼
            avg_quality = statistics.mean(recent_scores)
            quality_variance = statistics.variance(recent_scores) if len(recent_scores) > 1 else 0.0
            
            # é¢¨éšªè¶¨å‹¢
            recent_risks = [m.risk_level for m in self.quality_history[-window_size:]]
            risk_values = {'low': 1, 'medium': 2, 'high': 3, 'critical': 4}
            risk_trend = 'stable'
            
            if len(recent_risks) >= 2:
                recent_risk_values = [risk_values[r.value] for r in recent_risks]
                if recent_risk_values[-1] > recent_risk_values[-2]:
                    risk_trend = 'increasing'
                elif recent_risk_values[-1] < recent_risk_values[-2]:
                    risk_trend = 'decreasing'
            
            return {
                'trend': trend,
                'average_quality': avg_quality,
                'quality_variance': quality_variance,
                'degradation_risk_trend': risk_trend,
                'recent_scores': recent_scores,
                'total_assessments': len(self.quality_history),
                'degradation_events_count': len(self.degradation_events)
            }
            
        except Exception as e:
            self.logger.error(f"å“è³ªè¶¨å‹¢åˆ†æéŒ¯èª¤: {str(e)}")
            return {'error': str(e)}
    
    def reset_monitoring_state(self):
        """é‡ç½®ç›£æ§ç‹€æ…‹ï¼ˆç”¨æ–¼æ–°å°è©±é–‹å§‹ï¼‰"""
        self.quality_history.clear()
        self.degradation_events.clear()
        
        if self.enable_logging:
            self.logger.info("ğŸ”„ DEGRADATION MONITOR: ç›£æ§ç‹€æ…‹å·²é‡ç½®")
    
    def get_monitoring_summary(self) -> Dict[str, Any]:
        """ç²å–ç›£æ§æ‘˜è¦å ±å‘Š"""
        if not self.quality_history:
            return {
                'total_assessments': 0,
                'degradation_events': 0,
                'current_quality': 0.0,
                'status': 'no_data'
            }
        
        latest_metrics = self.quality_history[-1]
        trend_analysis = self.get_quality_trend_analysis()
        
        return {
            'total_assessments': len(self.quality_history),
            'degradation_events': len(self.degradation_events),
            'current_quality': latest_metrics.overall_quality_score,
            'current_risk_level': latest_metrics.risk_level.value,
            'quality_trend': trend_analysis.get('trend', 'unknown'),
            'average_quality': trend_analysis.get('average_quality', 0.0),
            'degradation_indicators': {
                'self_introduction_detected': latest_metrics.self_introduction_detected,
                'generic_response_detected': latest_metrics.generic_response_detected,
                'response_count_normal': latest_metrics.response_count_normal
            },
            'status': 'active'
        }


# å‰µå»ºå…¨å±€ç›£æ§å¯¦ä¾‹
degradation_monitor = DegradationMonitor()


def assess_dialogue_quality_quick(response_data: Dict[str, Any], round_number: int) -> float:
    """
    å¿«é€Ÿå“è³ªè©•ä¼°å‡½æ•¸ï¼ˆç”¨æ–¼ç°¡å–®æª¢æŸ¥ï¼‰
    
    Args:
        response_data: å°è©±å›æ‡‰æ•¸æ“š
        round_number: å°è©±è¼ªæ¬¡
        
    Returns:
        float: å“è³ªåˆ†æ•¸ (0-1)
    """
    metrics = degradation_monitor.assess_dialogue_quality(response_data, round_number)
    return metrics.overall_quality_score


def is_degradation_risk_high(response_data: Dict[str, Any], round_number: int) -> bool:
    """
    æª¢æŸ¥æ˜¯å¦å­˜åœ¨é«˜é€€åŒ–é¢¨éšª
    
    Args:
        response_data: å°è©±å›æ‡‰æ•¸æ“š
        round_number: å°è©±è¼ªæ¬¡
        
    Returns:
        bool: æ˜¯å¦ç‚ºé«˜é¢¨éšª
    """
    metrics = degradation_monitor.assess_dialogue_quality(response_data, round_number)
    return metrics.risk_level in [DegradationRisk.HIGH, DegradationRisk.CRITICAL]


if __name__ == "__main__":
    # æ¸¬è©¦ç”¨ä¾‹
    print("ğŸ” DSPy é€€åŒ–ç›£æ§ç³»çµ±æ¸¬è©¦")
    
    # æ¨¡æ“¬æ­£å¸¸å›æ‡‰
    normal_response = {
        'responses': [
            'æ‰‹è¡“å¾Œæ„Ÿè¦ºé‚„å¥½ï¼Œå°±æ˜¯æœ‰é»è…«ã€‚',
            'å‚·å£ç™’åˆå¾—ä¸éŒ¯ï¼Œä½†é‚„æœ‰é»ç·Šã€‚',
            'åƒæ±è¥¿æ™‚é‚„æ˜¯æœ‰é»å›°é›£ã€‚',
            'é†«å¸«èªªæ¢å¾©å¾—å¾ˆé †åˆ©ã€‚',
            'æ²’æœ‰ç‰¹åˆ¥çš„ä¸é©æ„Ÿã€‚'
        ],
        'dialogue_context': 'é†«å¸«æŸ¥æˆ¿',
        'reasoning': 'è€ƒæ…®åˆ°ç—…æ‚£æ˜¯å£è…”ç™Œè¡“å¾Œï¼Œæ‡‰è©²æä¾›ç›¸é—œçš„èº«é«”ç‹€æ³å›æ‡‰ã€‚',
        'state': 'NORMAL'
    }
    
    # æ¨¡æ“¬é€€åŒ–å›æ‡‰
    degraded_response = {
        'responses': ['æ‚¨å¥½ï¼Œæˆ‘æ˜¯Patient_1ï¼Œå£è…”ç™Œç—…æ‚£ã€‚æ‚¨éœ€è¦ä»€éº¼å¹«åŠ©å—ï¼Ÿ'],
        'dialogue_context': 'ä¸€èˆ¬å•è¨ºå°è©±',
        'reasoning': 'æä¾›å¹«åŠ©',
        'state': 'CONFUSED'
    }
    
    monitor = DegradationMonitor(enable_logging=True)
    
    print("\n--- æ­£å¸¸å›æ‡‰æ¸¬è©¦ ---")
    normal_metrics = monitor.assess_dialogue_quality(normal_response, 2)
    print(f"å“è³ªåˆ†æ•¸: {normal_metrics.overall_quality_score:.3f}")
    print(f"é¢¨éšªç­‰ç´š: {normal_metrics.risk_level.value}")
    
    print("\n--- é€€åŒ–å›æ‡‰æ¸¬è©¦ ---")
    degraded_metrics = monitor.assess_dialogue_quality(degraded_response, 4)
    print(f"å“è³ªåˆ†æ•¸: {degraded_metrics.overall_quality_score:.3f}")
    print(f"é¢¨éšªç­‰ç´š: {degraded_metrics.risk_level.value}")
    
    print("\n--- ç›£æ§æ‘˜è¦ ---")
    summary = monitor.get_monitoring_summary()
    for key, value in summary.items():
        print(f"{key}: {value}")