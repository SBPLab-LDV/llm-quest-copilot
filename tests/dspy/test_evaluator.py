#!/usr/bin/env python3
"""
æ¸¬è©¦ DSPy è©•ä¼°å™¨
"""

import sys
sys.path.insert(0, '/app')

def test_evaluator_creation():
    """æ¸¬è©¦è©•ä¼°å™¨å‰µå»º"""
    print("ğŸ§ª æ¸¬è©¦ DSPy è©•ä¼°å™¨å‰µå»º...")
    
    try:
        from src.core.dspy.evaluator import DSPyEvaluator
        
        # å‰µå»ºè©•ä¼°å™¨
        print("\n1. å‰µå»ºè©•ä¼°å™¨:")
        evaluator = DSPyEvaluator()
        print("  âœ… è©•ä¼°å™¨å‰µå»ºæˆåŠŸ")
        
        # æª¢æŸ¥è©•ä¼°å™¨å±¬æ€§
        print("\n2. æª¢æŸ¥è©•ä¼°å™¨å±¬æ€§:")
        assert hasattr(evaluator, 'metrics'), "ç¼ºå°‘ metrics å±¬æ€§"
        assert hasattr(evaluator, 'stats'), "ç¼ºå°‘ stats å±¬æ€§"
        assert hasattr(evaluator, 'evaluation_history'), "ç¼ºå°‘ evaluation_history å±¬æ€§"
        print("  âœ… åŸºæœ¬å±¬æ€§æ­£å¸¸")
        
        # æª¢æŸ¥å¯ç”¨æŒ‡æ¨™
        print("\n3. å¯ç”¨è©•ä¼°æŒ‡æ¨™:")
        available_metrics = list(evaluator.metrics.keys())
        print(f"  æŒ‡æ¨™åˆ—è¡¨: {available_metrics}")
        assert len(available_metrics) > 0, "æ‡‰è©²æœ‰å¯ç”¨æŒ‡æ¨™"
        print("  âœ… è©•ä¼°æŒ‡æ¨™æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ è©•ä¼°å™¨å‰µå»ºæ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_single_evaluation():
    """æ¸¬è©¦å–®å€‹è©•ä¼°"""
    print("\nğŸ“Š æ¸¬è©¦å–®å€‹é æ¸¬è©•ä¼°...")
    
    try:
        from src.core.dspy.evaluator import DSPyEvaluator
        
        evaluator = DSPyEvaluator()
        
        # å‰µå»ºæ¨¡æ“¬é æ¸¬çµæœ
        mock_prediction = type('MockPrediction', (), {
            'responses': ['æˆ‘å¾ˆå¥½', 'ä»Šå¤©æ„Ÿè¦ºä¸éŒ¯', 'è¬è¬é—œå¿ƒ'],
            'state': 'NORMAL',
            'dialogue_context': 'ä¸€èˆ¬å°è©±'
        })()
        
        # åŸ·è¡Œè©•ä¼°
        evaluation_result = evaluator.evaluate_prediction(
            user_input="ä½ ä»Šå¤©æ„Ÿè¦ºå¦‚ä½•ï¼Ÿ",
            prediction=mock_prediction
        )
        
        print(f"  ç¸½åˆ†: {evaluation_result['overall_score']:.2f}")
        print(f"  è©•ä¼°æŒ‡æ¨™æ•¸: {len(evaluation_result['scores'])}")
        
        # æª¢æŸ¥çµæœæ ¼å¼
        assert 'overall_score' in evaluation_result, "ç¼ºå°‘ç¸½åˆ†"
        assert 'scores' in evaluation_result, "ç¼ºå°‘è©³ç´°åˆ†æ•¸"
        assert isinstance(evaluation_result['scores'], dict), "åˆ†æ•¸æ‡‰è©²æ˜¯å­—å…¸"
        
        # æª¢æŸ¥åˆ†æ•¸ç¯„åœ
        for metric, score in evaluation_result['scores'].items():
            assert 0.0 <= score <= 1.0, f"åˆ†æ•¸ {metric}={score} è¶…å‡ºç¯„åœ"
        
        print("  âœ… å–®å€‹è©•ä¼°æ­£å¸¸")
        return True
        
    except Exception as e:
        print(f"âŒ å–®å€‹è©•ä¼°æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_individual_metrics():
    """æ¸¬è©¦å€‹åˆ¥è©•ä¼°æŒ‡æ¨™"""
    print("\nğŸ”§ æ¸¬è©¦å€‹åˆ¥è©•ä¼°æŒ‡æ¨™...")
    
    try:
        from src.core.dspy.evaluator import DSPyEvaluator
        
        evaluator = DSPyEvaluator()
        
        # æ¸¬è©¦ç”¨ä¾‹
        test_cases = [
            {
                'name': 'å®Œæ•´å›æ‡‰',
                'prediction': type('Pred', (), {
                    'responses': ['æˆ‘å¾ˆå¥½', 'æ„Ÿè¦ºä¸éŒ¯', 'è¬è¬'],
                    'state': 'NORMAL',
                    'dialogue_context': 'å•å€™'
                })(),
                'input': 'ä½ å¥½å—ï¼Ÿ'
            },
            {
                'name': 'ç©ºå›æ‡‰',
                'prediction': type('Pred', (), {
                    'responses': [],
                    'state': 'CONFUSED',
                    'dialogue_context': ''
                })(),
                'input': 'æ¸¬è©¦'
            }
        ]
        
        successful_tests = 0
        
        for test_case in test_cases:
            print(f"\n  æ¸¬è©¦æ¡ˆä¾‹: {test_case['name']}")
            
            try:
                # æ¸¬è©¦å›æ‡‰å“è³ª
                quality_score = evaluator._evaluate_response_quality(
                    test_case['input'], test_case['prediction']
                )
                print(f"    å›æ‡‰å“è³ª: {quality_score:.2f}")
                
                # æ¸¬è©¦ç‹€æ…‹ä¸€è‡´æ€§
                state_score = evaluator._evaluate_state_consistency(
                    test_case['input'], test_case['prediction']
                )
                print(f"    ç‹€æ…‹ä¸€è‡´æ€§: {state_score:.2f}")
                
                # æ¸¬è©¦å¤šæ¨£æ€§
                diversity_score = evaluator._evaluate_diversity(
                    test_case['input'], test_case['prediction']
                )
                print(f"    å›æ‡‰å¤šæ¨£æ€§: {diversity_score:.2f}")
                
                successful_tests += 1
                
            except Exception as e:
                print(f"    âŒ å¤±æ•—: {e}")
        
        print(f"\n  æˆåŠŸæ¸¬è©¦: {successful_tests}/{len(test_cases)}")
        return successful_tests > 0
        
    except Exception as e:
        print(f"âŒ å€‹åˆ¥æŒ‡æ¨™æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_evaluation_statistics():
    """æ¸¬è©¦è©•ä¼°çµ±è¨ˆ"""
    print("\nğŸ“ˆ æ¸¬è©¦è©•ä¼°çµ±è¨ˆ...")
    
    try:
        from src.core.dspy.evaluator import DSPyEvaluator
        
        evaluator = DSPyEvaluator()
        
        # åŸ·è¡Œå¹¾æ¬¡è©•ä¼°
        mock_prediction = type('MockPrediction', (), {
            'responses': ['å›æ‡‰1', 'å›æ‡‰2'],
            'state': 'NORMAL',
            'dialogue_context': 'æ¸¬è©¦'
        })()
        
        for i in range(3):
            evaluator.evaluate_prediction(
                user_input=f"æ¸¬è©¦è¼¸å…¥ {i}",
                prediction=mock_prediction
            )
        
        # æª¢æŸ¥çµ±è¨ˆ
        stats = evaluator.get_evaluation_statistics()
        
        print(f"  ç¸½è©•ä¼°æ¬¡æ•¸: {stats['total_evaluations']}")
        assert stats['total_evaluations'] == 3, f"è©•ä¼°æ¬¡æ•¸æ‡‰ç‚º3ï¼Œä½†å¾—åˆ° {stats['total_evaluations']}"
        
        # æª¢æŸ¥æ­·å²è¨˜éŒ„
        recent = evaluator.get_recent_evaluations(limit=2)
        print(f"  æœ€è¿‘è©•ä¼°è¨˜éŒ„: {len(recent)} ç­†")
        assert len(recent) == 2, f"æœ€è¿‘è¨˜éŒ„æ‡‰ç‚º2ç­†ï¼Œä½†å¾—åˆ° {len(recent)}"
        
        print("  âœ… çµ±è¨ˆåŠŸèƒ½æ­£å¸¸")
        return True
        
    except Exception as e:
        print(f"âŒ çµ±è¨ˆæ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_evaluation_edge_cases():
    """æ¸¬è©¦è©•ä¼°é‚Šç•Œæƒ…æ³"""
    print("\nğŸ§ æ¸¬è©¦è©•ä¼°é‚Šç•Œæƒ…æ³...")
    
    try:
        from src.core.dspy.evaluator import DSPyEvaluator
        
        evaluator = DSPyEvaluator()
        
        # æ¸¬è©¦ç©ºé æ¸¬
        empty_prediction = type('EmptyPrediction', (), {})()
        
        result = evaluator.evaluate_prediction(
            user_input="æ¸¬è©¦",
            prediction=empty_prediction
        )
        
        print(f"  ç©ºé æ¸¬è©•ä¼°åˆ†æ•¸: {result['overall_score']:.2f}")
        assert result['overall_score'] >= 0.0, "åˆ†æ•¸ä¸æ‡‰ç‚ºè² "
        
        # æ¸¬è©¦ç„¡æ•ˆç‹€æ…‹
        invalid_state_prediction = type('InvalidPrediction', (), {
            'responses': ['æ¸¬è©¦å›æ‡‰'],
            'state': 'INVALID_STATE',
            'dialogue_context': 'æ¸¬è©¦'
        })()
        
        result2 = evaluator.evaluate_prediction(
            user_input="æ¸¬è©¦",
            prediction=invalid_state_prediction
        )
        
        print(f"  ç„¡æ•ˆç‹€æ…‹è©•ä¼°åˆ†æ•¸: {result2['overall_score']:.2f}")
        
        print("  âœ… é‚Šç•Œæƒ…æ³è™•ç†æ­£å¸¸")
        return True
        
    except Exception as e:
        print(f"âŒ é‚Šç•Œæƒ…æ³æ¸¬è©¦å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    test1 = test_evaluator_creation()
    test2 = test_single_evaluation()
    test3 = test_individual_metrics()
    test4 = test_evaluation_statistics()
    test5 = test_evaluation_edge_cases()
    
    success_count = sum([test1, test2, test3, test4, test5])
    total_tests = 5
    
    print(f"\nğŸ“‹ æ¸¬è©¦ç¸½çµ: {success_count}/{total_tests} é€šé")
    
    if success_count >= total_tests * 0.8:
        print("âœ… è©•ä¼°å™¨æ¸¬è©¦é€šé")
    else:
        print("âš ï¸  è©•ä¼°å™¨æ¸¬è©¦éƒ¨åˆ†é€šé")